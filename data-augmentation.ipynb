{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc98cee9",
   "metadata": {
    "papermill": {
     "duration": 0.008025,
     "end_time": "2025-02-04T10:36:04.255296",
     "exception": false,
     "start_time": "2025-02-04T10:36:04.247271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5caeb3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T10:36:04.269513Z",
     "iopub.status.busy": "2025-02-04T10:36:04.269097Z",
     "iopub.status.idle": "2025-02-04T10:36:13.601997Z",
     "shell.execute_reply": "2025-02-04T10:36:13.600897Z"
    },
    "papermill": {
     "duration": 9.342841,
     "end_time": "2025-02-04T10:36:13.604780",
     "exception": false,
     "start_time": "2025-02-04T10:36:04.261939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install transformers timm albumentations --quiet\n",
    "! pip install ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378e26ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T10:36:13.619254Z",
     "iopub.status.busy": "2025-02-04T10:36:13.618890Z",
     "iopub.status.idle": "2025-02-04T10:36:41.441276Z",
     "shell.execute_reply": "2025-02-04T10:36:41.440245Z"
    },
    "papermill": {
     "duration": 27.831544,
     "end_time": "2025-02-04T10:36:41.443015",
     "exception": false,
     "start_time": "2025-02-04T10:36:13.611471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Numerical and Data Handling Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image Processing Libraries\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Plotting and Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch and Torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Transformers for Vision Models\n",
    "from transformers import ViTConfig, ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "# Utility Libraries\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# jupyter nbextension enable --py widgetsnbextension\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7fa35",
   "metadata": {
    "papermill": {
     "duration": 0.005825,
     "end_time": "2025-02-04T10:36:41.455237",
     "exception": false,
     "start_time": "2025-02-04T10:36:41.449412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing & Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe90d18",
   "metadata": {
    "papermill": {
     "duration": 0.005617,
     "end_time": "2025-02-04T10:36:41.466716",
     "exception": false,
     "start_time": "2025-02-04T10:36:41.461099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Split the Dataset by 80 / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55abd507",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T10:36:41.480119Z",
     "iopub.status.busy": "2025-02-04T10:36:41.479445Z",
     "iopub.status.idle": "2025-02-04T10:36:54.655610Z",
     "shell.execute_reply": "2025-02-04T10:36:54.654451Z"
    },
    "papermill": {
     "duration": 13.184869,
     "end_time": "2025-02-04T10:36:54.657439",
     "exception": false,
     "start_time": "2025-02-04T10:36:41.472570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset reorganization complete. New train and validation sets are saved in /kaggle/working/New_Split_Dataset.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Path to the dataset\n",
    "base_dir = '/kaggle/input/bdma-07-competition/BDMA7_project_files'\n",
    "train_dir = os.path.join(base_dir, 'train_images')\n",
    "val_dir = os.path.join(base_dir, 'val_images')\n",
    "output_base_dir = '/kaggle/working/New_Split_Dataset'  # Kaggle working directory for output\n",
    "new_train_dir = os.path.join(output_base_dir, 'train_images')\n",
    "new_val_dir = os.path.join(output_base_dir, 'val_images')\n",
    "\n",
    "# Create new directories for reorganized data\n",
    "os.makedirs(new_train_dir, exist_ok=True)\n",
    "os.makedirs(new_val_dir, exist_ok=True)\n",
    "\n",
    "# Reorganize each class\n",
    "for class_name in os.listdir(train_dir):\n",
    "    class_train_path = os.path.join(train_dir, class_name)\n",
    "    class_val_path = os.path.join(val_dir, class_name)\n",
    "    \n",
    "    # Combine train and validation images for splitting\n",
    "    all_images = []\n",
    "    if os.path.isdir(class_train_path):\n",
    "        all_images.extend([os.path.join(class_train_path, img) for img in os.listdir(class_train_path)])\n",
    "    if os.path.isdir(class_val_path):\n",
    "        all_images.extend([os.path.join(class_val_path, img) for img in os.listdir(class_val_path)])\n",
    "    \n",
    "    # Split into 80% train, 20% validation\n",
    "    train_images, val_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create class directories in new_train_dir and new_val_dir\n",
    "    new_class_train_path = os.path.join(new_train_dir, class_name)\n",
    "    new_class_val_path = os.path.join(new_val_dir, class_name)\n",
    "    os.makedirs(new_class_train_path, exist_ok=True)\n",
    "    os.makedirs(new_class_val_path, exist_ok=True)\n",
    "    \n",
    "    # Move images to respective directories\n",
    "    for img_path in train_images:\n",
    "        shutil.copy(img_path, new_class_train_path)\n",
    "    for img_path in val_images:\n",
    "        shutil.copy(img_path, new_class_val_path)\n",
    "\n",
    "print(f\"Dataset reorganization complete. New train and validation sets are saved in {output_base_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10abe7",
   "metadata": {
    "papermill": {
     "duration": 0.006236,
     "end_time": "2025-02-04T10:36:54.670251",
     "exception": false,
     "start_time": "2025-02-04T10:36:54.664015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Crop Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8591742a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T10:36:54.684111Z",
     "iopub.status.busy": "2025-02-04T10:36:54.683729Z",
     "iopub.status.idle": "2025-02-04T11:07:59.192176Z",
     "shell.execute_reply": "2025-02-04T11:07:59.190892Z"
    },
    "papermill": {
     "duration": 1864.517709,
     "end_time": "2025-02-04T11:07:59.194173",
     "exception": false,
     "start_time": "2025-02-04T10:36:54.676464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:00<00:00, 179MB/s]\n",
      "Processing Images:  11%|█         | 44/400 [03:24<27:11,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bird found in 79b7fa19-8d89-4fb7-8137-a59165f64148.jpg, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  19%|█▉        | 75/400 [05:49<24:05,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bird found in ec77de43-938a-4c91-b8ad-913edea35b17.jpg, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  32%|███▏      | 126/400 [09:47<21:29,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bird found in d104a7ab-2ff1-4dac-bf2e-08dbf22f0145.jpg, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  43%|████▎     | 173/400 [13:36<17:53,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bird found in 175b9210-001f-4ccc-9cea-92ea32d3642b.jpg, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  52%|█████▏    | 206/400 [16:19<16:12,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bird found in 7ee575cf-4da6-44bf-a5cf-4d1a6bde1f9a.jpg, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  70%|███████   | 281/400 [22:09<08:27,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bird found in 43c2aa65-9ce0-40d6-af2d-eecc0594248d.jpg, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  83%|████████▎ | 333/400 [26:10<05:14,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bird found in a018c470-3b07-4bec-8416-9c6e40d09d6a.jpg, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  94%|█████████▍| 377/400 [29:18<01:45,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bird found in 8588952e-90f1-4f0e-bf3d-bded575d5fcd.jpg, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  95%|█████████▌| 380/400 [29:32<01:33,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No bird found in d7e74bb2-7f80-4a76-b191-d9318e873caf.jpg, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 400/400 [31:02<00:00,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete! Cropped images saved in: /kaggle/working/Augmented_Processed_CropImage_Dataset/test_images/mistery_cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Paths\n",
    "original_images_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/test_images/mistery_cat\"\n",
    "no_background_images_dir = \"/kaggle/input/augmented-processed-removedbackground-birddataset/Augmented_Processed_RemovedBackground_Dataset/test_images/mistery_cat\"\n",
    "output_dir = \"/kaggle/working/Augmented_Processed_CropImage_Dataset/test_images/mistery_cat\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Extra padding around the detected bird\n",
    "padding = 50\n",
    "\n",
    "# Pre-trained model (Faster R-CNN for object detection)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Get all image files\n",
    "image_files = [f for f in os.listdir(no_background_images_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "for image_file in tqdm(image_files, desc=\"Processing Images\"):\n",
    "    # Load images\n",
    "    original_path = os.path.join(original_images_dir, image_file)\n",
    "    output_path = os.path.join(output_dir, image_file)\n",
    "    \n",
    "    original_img = cv2.imread(original_path)\n",
    "    \n",
    "    if original_img is None:\n",
    "        print(f\"Skipping {image_file} due to loading error.\")\n",
    "        continue\n",
    "    \n",
    "    # Convert original image to PIL format for model processing\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB))\n",
    "    img_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Detect objects\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img_tensor)\n",
    "\n",
    "    # Extract bird bounding boxes (use label 16 for birds in COCO dataset)\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "\n",
    "    bird_boxes = [box for box, score, label in zip(boxes, scores, labels) if label == 16 and score > 0.5]\n",
    "\n",
    "    if not bird_boxes:\n",
    "        print(f\"No bird found in {image_file}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Use the first detected bird box (highest confidence)\n",
    "    x_min, y_min, x_max, y_max = bird_boxes[0]\n",
    "    x_min = int(max(0, x_min - padding))\n",
    "    y_min = int(max(0, y_min - padding))\n",
    "    x_max = int(min(original_img.shape[1], x_max + padding))\n",
    "    y_max = int(min(original_img.shape[0], y_max + padding))\n",
    "\n",
    "    # Crop and save the image\n",
    "    cropped_img = original_img[y_min:y_max, x_min:x_max]\n",
    "    cv2.imwrite(output_path, cropped_img)\n",
    "\n",
    "print(\"Processing complete! Cropped images saved in:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040c0cb",
   "metadata": {
    "papermill": {
     "duration": 0.031025,
     "end_time": "2025-02-04T11:07:59.258298",
     "exception": false,
     "start_time": "2025-02-04T11:07:59.227273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31639852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:07:59.318564Z",
     "iopub.status.busy": "2025-02-04T11:07:59.318173Z",
     "iopub.status.idle": "2025-02-04T11:07:59.322425Z",
     "shell.execute_reply": "2025-02-04T11:07:59.321639Z"
    },
    "papermill": {
     "duration": 0.035451,
     "end_time": "2025-02-04T11:07:59.323735",
     "exception": false,
     "start_time": "2025-02-04T11:07:59.288284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Data augmentation (rotation)\n",
    "# train_dir = \"/kaggle/working/processed_images/train_images\"\n",
    "# output_dir = \"/kaggle/working/preprocessed_images\"\n",
    "\n",
    "# # Image processing parameters\n",
    "# img_size = (128, 128)  # Resize dimensions\n",
    "# rotation_angles = [-15, -10, -5, 5, 10, 15]  # Rotation angles\n",
    "\n",
    "# # Create output directory\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Data augmentation and saving\n",
    "# def preprocess_and_augment_images(input_dir, output_dir, img_size, angles):\n",
    "#     \"\"\"\n",
    "#     Perform preprocessing (resize) and augmentation (rotation) on images.\n",
    "#     \"\"\"\n",
    "#     for class_folder in tqdm(os.listdir(input_dir), desc=\"Processing Classes\"):\n",
    "#         class_path = os.path.join(input_dir, class_folder)\n",
    "#         if not os.path.isdir(class_path):\n",
    "#             continue\n",
    "\n",
    "#         # Create output directory for each class\n",
    "#         class_output_path = os.path.join(output_dir, class_folder)\n",
    "#         os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "#         # Process each image\n",
    "#         for img_file in os.listdir(class_path):\n",
    "#             img_path = os.path.join(class_path, img_file)\n",
    "#             img = cv2.imread(img_path)\n",
    "\n",
    "#             if img is None:\n",
    "#                 print(f\"Warning: Unable to read image {img_path}\")\n",
    "#                 continue\n",
    "\n",
    "#             # Resize the image\n",
    "#             resized_img = cv2.resize(img, img_size)\n",
    "\n",
    "#             # Save the resized original image\n",
    "#             base_name = os.path.splitext(img_file)[0]\n",
    "#             cv2.imwrite(os.path.join(class_output_path, f\"{base_name}_original.jpg\"), resized_img)\n",
    "\n",
    "#             # Data augmentation (rotation)\n",
    "#             for angle in angles:\n",
    "#                 # Calculate the rotation matrix\n",
    "#                 h, w = resized_img.shape[:2]\n",
    "#                 center = (w // 2, h // 2)\n",
    "#                 rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "#                 # Rotate the image\n",
    "#                 rotated_img = cv2.warpAffine(resized_img, rotation_matrix, (w, h))\n",
    "\n",
    "#                 # Save the augmented image\n",
    "#                 augmented_name = f\"{base_name}_rotated_{angle}.jpg\"\n",
    "#                 cv2.imwrite(os.path.join(class_output_path, augmented_name), rotated_img)\n",
    "\n",
    "# # Execute preprocessing and augmentation\n",
    "# preprocess_and_augment_images(train_dir, output_dir, img_size, rotation_angles)\n",
    "\n",
    "# print(\"Data preprocessing and augmentation complete. Files saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a82b888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:07:59.389800Z",
     "iopub.status.busy": "2025-02-04T11:07:59.389373Z",
     "iopub.status.idle": "2025-02-04T11:07:59.394542Z",
     "shell.execute_reply": "2025-02-04T11:07:59.393285Z"
    },
    "papermill": {
     "duration": 0.043727,
     "end_time": "2025-02-04T11:07:59.396123",
     "exception": false,
     "start_time": "2025-02-04T11:07:59.352396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Data augmentation (flip)\n",
    "# def flip_image(image, flip_code):\n",
    "#     return cv2.flip(image, flip_code)\n",
    "\n",
    "# train_dir = \"/kaggle/working/preprocessed_images\"\n",
    "# output_dir = \"/kaggle/working/preprocessed02_images\"\n",
    "# # Define a list of flip codes\n",
    "# # flip_codes = [0, 1, -1]\n",
    "# flip_codes = [1]\n",
    "\n",
    "# # Iterate through subdirectories (classes) in the input directory\n",
    "# for root, dirs, files in os.walk(train_dir):\n",
    "#     for dir_name in dirs:\n",
    "#         input_class_dir = os.path.join(root, dir_name)\n",
    "#         output_class_dir = os.path.join(output_dir, dir_name)\n",
    "#         # Create the output directory if it doesn't exist\n",
    "#         if not os.path.exists(output_class_dir):\n",
    "#             os.makedirs(output_class_dir)\n",
    "#         # Iterate through files in the class directory\n",
    "#         for file in os.listdir(input_class_dir):\n",
    "#             if file.endswith(\".jpg\"):\n",
    "#                 input_file_path = os.path.join(input_class_dir, file)\n",
    "#                 # Read the image\n",
    "#                 image = cv2.imread(input_file_path)\n",
    "                \n",
    "#                 # Copy the original image to the output directory\n",
    "#                 output_original_file_path = os.path.join(output_class_dir, file)\n",
    "#                 cv2.imwrite(output_original_file_path, image)\n",
    "                \n",
    "#                 # Generate flipped images for each flip operation\n",
    "#                 for flip_code in flip_codes:\n",
    "#                     # Generate a new file name\n",
    "#                     base_name = os.path.splitext(file)[0]\n",
    "#                     output_file_name = f\"{base_name}_flipped_{flip_code}.jpg\"\n",
    "#                     output_file_path = os.path.join(output_class_dir, output_file_name)\n",
    "#                     # Perform the flip operation on the image\n",
    "#                     flipped_image = flip_image(image, flip_code)\n",
    "#                     cv2.imwrite(output_file_path, flipped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9404a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:07:59.478419Z",
     "iopub.status.busy": "2025-02-04T11:07:59.478028Z",
     "iopub.status.idle": "2025-02-04T11:08:18.965662Z",
     "shell.execute_reply": "2025-02-04T11:08:18.964313Z"
    },
    "papermill": {
     "duration": 19.52535,
     "end_time": "2025-02-04T11:08:18.967306",
     "exception": false,
     "start_time": "2025-02-04T11:07:59.441956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train_images: 100%|██████████| 20/20 [00:14<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val_images: 100%|██████████| 20/20 [00:01<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_images: 100%|██████████| 1/1 [00:03<00:00,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Augmented and preprocessed data saved to:\n",
      "Train: /kaggle/working/Augmented_Processed_Dataset/train_images\n",
      "Validation: /kaggle/working/Augmented_Processed_Dataset/val_images\n",
      "Test: /kaggle/working/Augmented_Processed_Dataset/test_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Paths to datasets\n",
    "train_dir = \"/kaggle/working/New_Split_Dataset/train_images\"\n",
    "val_dir = \"/kaggle/working/New_Split_Dataset/val_images\"\n",
    "test_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/test_images\"\n",
    "augmented_train_dir = \"/kaggle/working/Augmented_Processed_Dataset/train_images\"\n",
    "augmented_val_dir = \"/kaggle/working/Augmented_Processed_Dataset/val_images\"\n",
    "augmented_test_dir = \"/kaggle/working/Augmented_Processed_Dataset/test_images\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(augmented_train_dir, exist_ok=True)\n",
    "os.makedirs(augmented_val_dir, exist_ok=True)\n",
    "os.makedirs(augmented_test_dir, exist_ok=True)\n",
    "\n",
    "# Define transformations for training, validation, and test sets\n",
    "def get_train_augmentations():\n",
    "    return T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.RandomCrop((224, 224)),\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomRotation(degrees=15),\n",
    "        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        T.ToTensor(),  # Convert to Tensor\n",
    "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize\n",
    "    ])\n",
    "\n",
    "def get_val_preprocessing():\n",
    "    return T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize\n",
    "    ])\n",
    "\n",
    "# De-normalization function to convert tensors back to human-readable format\n",
    "def denormalize(tensor, mean, std):\n",
    "    \"\"\"\n",
    "    Reverse the normalization process to bring pixel values back to [0, 1].\n",
    "    \"\"\"\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Denormalize: t = t * std + mean\n",
    "    return tensor\n",
    "\n",
    "# Function to process images and save results\n",
    "def process_images(input_dir, output_dir, transform, augment=False):\n",
    "    for class_folder in tqdm(os.listdir(input_dir), desc=f\"Processing {os.path.basename(input_dir)}\"):\n",
    "        class_path = os.path.join(input_dir, class_folder)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        # Create output directory for each class\n",
    "        class_output_path = os.path.join(output_dir, class_folder)\n",
    "        os.makedirs(class_output_path, exist_ok=True)\n",
    "        \n",
    "        # Process each image in the class directory\n",
    "        for img_file in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {img_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Apply transformations\n",
    "            transformed_img = transform(img)\n",
    "            base_name = os.path.splitext(img_file)[0]\n",
    "\n",
    "            if augment:\n",
    "                # Augmented images for training\n",
    "                output_file = os.path.join(class_output_path, f\"{base_name}_augmented.jpg\")\n",
    "            else:\n",
    "                # Preprocessed images for validation and test\n",
    "                output_file = os.path.join(class_output_path, f\"{base_name}.jpg\")\n",
    "            \n",
    "            # Denormalize before saving\n",
    "            denormalized_img = denormalize(transformed_img.clone(), mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "            denormalized_img = denormalized_img.clamp(0, 1)  # Ensure pixel values are within [0, 1]\n",
    "\n",
    "            # Convert back to PIL image and save\n",
    "            transformed_pil = T.ToPILImage()(denormalized_img)\n",
    "            transformed_pil.save(output_file)\n",
    "\n",
    "# Process training set with data augmentation\n",
    "print(\"Augmenting training dataset...\")\n",
    "train_augmentations = get_train_augmentations()\n",
    "process_images(train_dir, augmented_train_dir, train_augmentations, augment=True)\n",
    "\n",
    "# Process validation set without augmentation\n",
    "print(\"Processing validation dataset...\")\n",
    "val_preprocessing = get_val_preprocessing()\n",
    "process_images(val_dir, augmented_val_dir, val_preprocessing, augment=False)\n",
    "\n",
    "# Process test set without augmentation\n",
    "print(\"Processing test dataset...\")\n",
    "process_images(test_dir, augmented_test_dir, val_preprocessing, augment=False)\n",
    "\n",
    "print(\"Data processing complete. Augmented and preprocessed data saved to:\")\n",
    "print(f\"Train: {augmented_train_dir}\")\n",
    "print(f\"Validation: {augmented_val_dir}\")\n",
    "print(f\"Test: {augmented_test_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186dc8ba",
   "metadata": {
    "papermill": {
     "duration": 0.030939,
     "end_time": "2025-02-04T11:08:19.031381",
     "exception": false,
     "start_time": "2025-02-04T11:08:19.000442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocess by DeepLabV3 Remove Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d91e432",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:19.095467Z",
     "iopub.status.busy": "2025-02-04T11:08:19.094961Z",
     "iopub.status.idle": "2025-02-04T11:08:19.100394Z",
     "shell.execute_reply": "2025-02-04T11:08:19.099321Z"
    },
    "papermill": {
     "duration": 0.039541,
     "end_time": "2025-02-04T11:08:19.102076",
     "exception": false,
     "start_time": "2025-02-04T11:08:19.062535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torchvision.transforms as T\n",
    "# from PIL import Image\n",
    "# from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n",
    "# import numpy as np\n",
    "\n",
    "# # Set data paths\n",
    "# train_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/train_images\"\n",
    "# val_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/val_images\"\n",
    "# test_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/test_images/mistery_cat\"\n",
    "# output_dir = \"/kaggle/working/processed_images\"\n",
    "\n",
    "# # Create output directory\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Load the pretrained DeepLabV3+ model (ResNet101 backbone)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Load pretrained model with correct weights handling\n",
    "# weights = DeepLabV3_ResNet101_Weights.DEFAULT\n",
    "# model = deeplabv3_resnet101(weights=weights).to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # Image preprocessing (fixed \"mean\" and \"std\" issue)\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"Preprocess the input image for the DeepLabV3+ model.\"\"\"\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     transform = T.Compose([\n",
    "#         T.Resize((512, 512)),\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize(mean=weights.transforms().mean, std=weights.transforms().std),  # FIXED\n",
    "#     ])\n",
    "#     return transform(image).unsqueeze(0), image\n",
    "\n",
    "# # Post-processing: apply the segmentation mask to the original image\n",
    "# def apply_mask(image, mask):\n",
    "#     \"\"\"Apply the segmentation mask to the original image to remove the background.\"\"\"\n",
    "#     mask = mask.cpu().numpy()\n",
    "#     mask = (mask > 0.5).astype(np.uint8)  # Binarize the mask\n",
    "#     mask = Image.fromarray(mask * 255).resize(image.size, Image.BILINEAR)\n",
    "#     mask = np.array(mask) / 255\n",
    "#     image_np = np.array(image)\n",
    "#     result = (image_np * np.expand_dims(mask, axis=-1)).astype(np.uint8)\n",
    "#     return Image.fromarray(result)\n",
    "\n",
    "# def process_directory(input_dir, output_subdir):\n",
    "#     \"\"\"Process all images in a directory, including subdirectories, and save the results.\"\"\"\n",
    "#     output_path = os.path.join(output_dir, output_subdir)\n",
    "#     os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "#     print(f\"🔍 Removing images Background in: {input_dir}\")\n",
    "\n",
    "#     # Walk through all subdirectories and files\n",
    "#     for root, _, files in os.walk(input_dir):\n",
    "#         for filename in files:\n",
    "#             file_path = os.path.join(root, filename)\n",
    "\n",
    "#             # Process only image files\n",
    "#             if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "#                 # Recreate subdirectory structure in output\n",
    "#                 relative_path = os.path.relpath(root, input_dir)\n",
    "#                 sub_output_path = os.path.join(output_path, relative_path)\n",
    "#                 os.makedirs(sub_output_path, exist_ok=True)\n",
    "\n",
    "#                 try:\n",
    "#                     # Preprocess the image\n",
    "#                     input_tensor, original_image = preprocess_image(file_path)\n",
    "#                     input_tensor = input_tensor.to(device)\n",
    "\n",
    "#                     # Inference\n",
    "#                     with torch.no_grad():\n",
    "#                         output = model(input_tensor)['out']\n",
    "#                     mask = torch.argmax(output.squeeze(), dim=0)\n",
    "\n",
    "#                     # Apply the mask\n",
    "#                     result_image = apply_mask(original_image, mask)\n",
    "\n",
    "#                     # Save the result\n",
    "#                     result_path = os.path.join(sub_output_path, filename)\n",
    "#                     result_image.save(result_path)\n",
    "#                     # print(f\"✅ Processed and saved: {result_path}\")\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"❌ Error processing {file_path}: {e}\")  # Debugging output\n",
    "\n",
    "# # Process training, validation, and test datasets\n",
    "# print(\"Processing training images...\")\n",
    "# process_directory(train_dir, \"train_images\")\n",
    "\n",
    "# print(\"Processing validation images...\")\n",
    "# process_directory(val_dir, \"val_images\")\n",
    "\n",
    "# print(\"Processing test images...\")\n",
    "# process_directory(test_dir, \"test_images\")\n",
    "\n",
    "# print(f\"All images processed and saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b5e7139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:19.168290Z",
     "iopub.status.busy": "2025-02-04T11:08:19.167893Z",
     "iopub.status.idle": "2025-02-04T11:08:19.172908Z",
     "shell.execute_reply": "2025-02-04T11:08:19.171990Z"
    },
    "papermill": {
     "duration": 0.041003,
     "end_time": "2025-02-04T11:08:19.174450",
     "exception": false,
     "start_time": "2025-02-04T11:08:19.133447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torchvision.transforms as T\n",
    "# from PIL import Image\n",
    "# from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n",
    "# import numpy as np\n",
    "\n",
    "# # Paths\n",
    "# train_dir = \"/kaggle/working/New_Split_Dataset/train_images\"\n",
    "# augmented_train_dir = \"/kaggle/working/Augmented_Processed_Dataset/train_images\"\n",
    "# output_train_dir = \"/kaggle/working/Augmented_Processed_RemovedBackground_Dataset/train_images\"\n",
    "# val_dir = \"/kaggle/working/New_Split_Dataset/val_images\"\n",
    "# output_val_dir = \"/kaggle/working/Augmented_Processed_RemovedBackground_Dataset/val_images\"\n",
    "# test_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/test_images\"\n",
    "# output_test_dir = \"/kaggle/working/Augmented_Processed_RemovedBackground_Dataset/test_images\"\n",
    "\n",
    "# # Create output directories\n",
    "# os.makedirs(output_train_dir, exist_ok=True)\n",
    "# os.makedirs(output_val_dir, exist_ok=True)\n",
    "# os.makedirs(output_test_dir, exist_ok=True)\n",
    "\n",
    "# # Load pretrained DeepLabV3+ model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# weights = DeepLabV3_ResNet101_Weights.DEFAULT\n",
    "# model = deeplabv3_resnet101(weights=weights).to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # Image preprocessing\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"Preprocess the input image for the DeepLabV3+ model.\"\"\"\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     transform = T.Compose([\n",
    "#         T.Resize((512, 512)),\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize(mean=weights.transforms().mean, std=weights.transforms().std),\n",
    "#     ])\n",
    "#     return transform(image).unsqueeze(0), image\n",
    "\n",
    "# # Post-processing: apply the segmentation mask to the original image\n",
    "# def apply_mask(image, mask):\n",
    "#     \"\"\"Apply the segmentation mask to the original image to remove the background.\"\"\"\n",
    "#     mask = mask.cpu().numpy()\n",
    "#     mask = (mask > 0.5).astype(np.uint8)  # Binarize the mask\n",
    "#     mask = Image.fromarray(mask * 255).resize(image.size, Image.BILINEAR)\n",
    "#     mask = np.array(mask) / 255\n",
    "#     image_np = np.array(image)\n",
    "#     result = (image_np * np.expand_dims(mask, axis=-1)).astype(np.uint8)\n",
    "#     return Image.fromarray(result)\n",
    "\n",
    "# # Process images and remove background\n",
    "# def process_directory(input_dirs, output_dir):\n",
    "#     \"\"\"Process all images in multiple directories and save results.\"\"\"\n",
    "#     for input_dir in input_dirs:\n",
    "#         print(f\"Removing background from images in: {input_dir}\")\n",
    "#         for root, _, files in os.walk(input_dir):\n",
    "#             for filename in files:\n",
    "#                 file_path = os.path.join(root, filename)\n",
    "\n",
    "#                 # Process only image files\n",
    "#                 if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "#                     # Recreate subdirectory structure in output\n",
    "#                     relative_path = os.path.relpath(root, input_dir)\n",
    "#                     sub_output_path = os.path.join(output_dir, relative_path)\n",
    "#                     os.makedirs(sub_output_path, exist_ok=True)\n",
    "\n",
    "#                     try:\n",
    "#                         # Preprocess the image\n",
    "#                         input_tensor, original_image = preprocess_image(file_path)\n",
    "#                         input_tensor = input_tensor.to(device)\n",
    "\n",
    "#                         # Inference\n",
    "#                         with torch.no_grad():\n",
    "#                             output = model(input_tensor)['out']\n",
    "#                         mask = torch.argmax(output.squeeze(), dim=0)\n",
    "\n",
    "#                         # Apply the mask\n",
    "#                         result_image = apply_mask(original_image, mask)\n",
    "\n",
    "#                         # Save the result\n",
    "#                         result_path = os.path.join(sub_output_path, filename)\n",
    "#                         result_image.save(result_path)\n",
    "\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# # Process training set\n",
    "# process_directory([train_dir, augmented_train_dir], output_train_dir)\n",
    "\n",
    "# # Process validation set\n",
    "# process_directory([val_dir], output_val_dir)\n",
    "\n",
    "# # Process test set\n",
    "# process_directory([test_dir], output_test_dir)\n",
    "\n",
    "# print(f\"All images processed and saved in {output_train_dir}, {output_val_dir}, and {output_test_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e45d277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:19.241427Z",
     "iopub.status.busy": "2025-02-04T11:08:19.241033Z",
     "iopub.status.idle": "2025-02-04T11:08:29.339422Z",
     "shell.execute_reply": "2025-02-04T11:08:29.338264Z"
    },
    "papermill": {
     "duration": 10.135184,
     "end_time": "2025-02-04T11:08:29.341049",
     "exception": false,
     "start_time": "2025-02-04T11:08:19.205865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/Augmented_Processed_RemovedBackground_Dataset'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir = \"/kaggle/input/augmented-processed-removedbackground-birddataset/Augmented_Processed_RemovedBackground_Dataset\"\n",
    "output_dir = \"/kaggle/working/Augmented_Processed_RemovedBackground_Dataset\"\n",
    "\n",
    "shutil.copytree(input_dir, output_dir, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab28bd7",
   "metadata": {
    "papermill": {
     "duration": 0.062984,
     "end_time": "2025-02-04T11:08:29.437502",
     "exception": false,
     "start_time": "2025-02-04T11:08:29.374518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Vision Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ec2be80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:29.519293Z",
     "iopub.status.busy": "2025-02-04T11:08:29.518816Z",
     "iopub.status.idle": "2025-02-04T11:08:29.522887Z",
     "shell.execute_reply": "2025-02-04T11:08:29.521977Z"
    },
    "papermill": {
     "duration": 0.03807,
     "end_time": "2025-02-04T11:08:29.524584",
     "exception": false,
     "start_time": "2025-02-04T11:08:29.486514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dir = \"/kaggle/working/preprocessed02_images\"\n",
    "# val_dir = \"/kaggle/working/processed_images/val_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1904f4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:29.589326Z",
     "iopub.status.busy": "2025-02-04T11:08:29.588823Z",
     "iopub.status.idle": "2025-02-04T11:08:29.593382Z",
     "shell.execute_reply": "2025-02-04T11:08:29.592373Z"
    },
    "papermill": {
     "duration": 0.038917,
     "end_time": "2025-02-04T11:08:29.594874",
     "exception": false,
     "start_time": "2025-02-04T11:08:29.555957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # model_name = \"google/vit-base-patch16-224-in21k\"  # 224x224 Input，16x16 Chunk\n",
    "# # Change to ViT-Large\n",
    "# model_name = \"google/vit-large-patch16-224-in21k\"\n",
    "# processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "# model = ViTForImageClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=20,  # Number of Classes\n",
    "#     ignore_mismatched_sizes=True  \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72216864",
   "metadata": {
    "papermill": {
     "duration": 0.032852,
     "end_time": "2025-02-04T11:08:29.659258",
     "exception": false,
     "start_time": "2025-02-04T11:08:29.626406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Freeze feature layers of ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32b3e5c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:29.723230Z",
     "iopub.status.busy": "2025-02-04T11:08:29.722768Z",
     "iopub.status.idle": "2025-02-04T11:08:29.726882Z",
     "shell.execute_reply": "2025-02-04T11:08:29.725925Z"
    },
    "papermill": {
     "duration": 0.038362,
     "end_time": "2025-02-04T11:08:29.728631",
     "exception": false,
     "start_time": "2025-02-04T11:08:29.690269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Freeze all feature layers of ViT (train only the classification head)\n",
    "# for param in model.vit.parameters():\n",
    "#     # param.requires_grad = False\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Thaw the classification headers separately\n",
    "# for param in model.classifier.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e575b",
   "metadata": {
    "papermill": {
     "duration": 0.031915,
     "end_time": "2025-02-04T11:08:29.791850",
     "exception": false,
     "start_time": "2025-02-04T11:08:29.759935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Set layer-wise learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1e3127c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:29.858371Z",
     "iopub.status.busy": "2025-02-04T11:08:29.857994Z",
     "iopub.status.idle": "2025-02-04T11:08:29.861628Z",
     "shell.execute_reply": "2025-02-04T11:08:29.860795Z"
    },
    "papermill": {
     "duration": 0.037505,
     "end_time": "2025-02-04T11:08:29.863070",
     "exception": false,
     "start_time": "2025-02-04T11:08:29.825565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Set layer-wise learning rate (feature layer learning rate is lower)\n",
    "# optimizer = AdamW(\n",
    "#     [\n",
    "#         {\"params\": model.vit.parameters(), \"lr\": 1e-5},  # Feature extraction layer\n",
    "#         {\"params\": model.classifier.parameters(), \"lr\": 3e-4}  # Classification Header\n",
    "#     ],\n",
    "#     weight_decay=0.01\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99546a1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:29.928366Z",
     "iopub.status.busy": "2025-02-04T11:08:29.927989Z",
     "iopub.status.idle": "2025-02-04T11:08:29.931703Z",
     "shell.execute_reply": "2025-02-04T11:08:29.930694Z"
    },
    "papermill": {
     "duration": 0.038052,
     "end_time": "2025-02-04T11:08:29.933402",
     "exception": false,
     "start_time": "2025-02-04T11:08:29.895350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Verify that the classification layer is randomly initialized (correct state should be True)\n",
    "# print(model.classifier.weight.mean().item())  # Should be close to 0 (normal distribution initialization)\n",
    "# print(model.classifier.bias.mean().item())    # Should be close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a495545",
   "metadata": {
    "papermill": {
     "duration": 0.032131,
     "end_time": "2025-02-04T11:08:29.997571",
     "exception": false,
     "start_time": "2025-02-04T11:08:29.965440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check Submission Classes Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e399b32a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:30.062629Z",
     "iopub.status.busy": "2025-02-04T11:08:30.062291Z",
     "iopub.status.idle": "2025-02-04T11:08:30.066276Z",
     "shell.execute_reply": "2025-02-04T11:08:30.065361Z"
    },
    "papermill": {
     "duration": 0.037892,
     "end_time": "2025-02-04T11:08:30.067766",
     "exception": false,
     "start_time": "2025-02-04T11:08:30.029874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission_class_order = [\n",
    "#     'Groove_billed_Ani',\n",
    "#     'Red_winged_Blackbird',\n",
    "#     'Rusty_Blackbird',\n",
    "#     'Gray_Catbird',\n",
    "#     'Brandt_Cormorant',\n",
    "#     'Eastern_Towhee',\n",
    "#     'Indigo_Bunting',\n",
    "#     'Brewer_Blackbird',\n",
    "#     'Painted_Bunting',\n",
    "#     'Bobolink',\n",
    "#     'Lazuli_Bunting',\n",
    "#     'Yellow_headed_Blackbird',\n",
    "#     'American_Crow',\n",
    "#     'Fish_Crow',\n",
    "#     'Brown_Creeper',\n",
    "#     'Yellow_billed_Cuckoo',\n",
    "#     'Yellow_breasted_Chat',\n",
    "#     'Black_billed_Cuckoo',\n",
    "#     'Gray_crowned_Rosy_Finch',\n",
    "#     'Bronzed_Cowbird'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56d86810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:30.134478Z",
     "iopub.status.busy": "2025-02-04T11:08:30.134112Z",
     "iopub.status.idle": "2025-02-04T11:08:30.138709Z",
     "shell.execute_reply": "2025-02-04T11:08:30.137891Z"
    },
    "papermill": {
     "duration": 0.040138,
     "end_time": "2025-02-04T11:08:30.140221",
     "exception": false,
     "start_time": "2025-02-04T11:08:30.100083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class BirdDataset(Dataset):\n",
    "#     def __init__(self, main_dir, transform=None):\n",
    "#         self.dataset = ImageFolder(root=main_dir, transform=transform)\n",
    "#         self.class_to_idx = self.dataset.class_to_idx\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         image, label = self.dataset[idx]\n",
    "#         return image, label\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.RandomCrop(224),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "# ])\n",
    "\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "# ])\n",
    "\n",
    "# train_dataset = BirdDataset(train_dir, transform=train_transform)\n",
    "# val_dataset = BirdDataset(val_dir, transform=val_transform)\n",
    "\n",
    "# def validate_class_order(train_class_order, submission_order):\n",
    "#     \"\"\"Make sure the category names and order of both lists are exactly the same\"\"\"\n",
    "#     if len(train_class_order) != len(submission_order):\n",
    "#         raise ValueError(f\"The number of categories does not match! Training set: {len(train_class_order)}, Submission Requirements: {len(submission_order)}\")\n",
    "    \n",
    "#     for train_cls, sub_cls in zip(train_class_order, submission_order):\n",
    "#         if train_cls != sub_cls:\n",
    "#             raise ValueError(f\"Inconsistent order: training set '{train_cls}' vs Submission Requirements '{sub_cls}'\")\n",
    "#     return True\n",
    "\n",
    "# train_class_order = sorted(train_dataset.class_to_idx.keys())\n",
    "\n",
    "# try:\n",
    "#     validate_class_order(train_class_order, submission_class_order)\n",
    "# except ValueError as e:\n",
    "#     print(\"Category order inconsistency detected, automatically correcting...\")\n",
    "#     from torchvision.datasets import DatasetFolder\n",
    "    \n",
    "#     class OrderedImageFolder(DatasetFolder):\n",
    "#         \"\"\"Forces the data sets of categories to be loaded in a specified order\"\"\"\n",
    "#         def __init__(self, root, class_order, transform=None):\n",
    "#             self.class_order = class_order\n",
    "#             super().__init__(\n",
    "#                 root,\n",
    "#                 loader=lambda x: Image.open(x).convert(\"RGB\"),\n",
    "#                 extensions=('jpg', 'jpeg', 'png'),\n",
    "#                 transform=transform,\n",
    "#                 target_transform=None\n",
    "#             )\n",
    "            \n",
    "#         def find_classes(self, directory):\n",
    "#             classes = self.class_order \n",
    "#             class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "#             return classes, class_to_idx\n",
    "    \n",
    "#     train_dataset = OrderedImageFolder(\n",
    "#         train_dir, \n",
    "#         class_order=submission_class_order,\n",
    "#         transform=train_transform\n",
    "#     )\n",
    "#     val_dataset = OrderedImageFolder(\n",
    "#         val_dir,\n",
    "#         class_order=submission_class_order,\n",
    "#         transform=val_transform\n",
    "#     )\n",
    "    \n",
    "#     print(\"Corrected category order：\", train_dataset.classes)\n",
    "    \n",
    "# # Category index validation (ensuring consistency with submission format)\n",
    "# assert sorted(train_dataset.class_to_idx.keys()) == sorted(submission_class_order), \"Category order mismatch！\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f65423d",
   "metadata": {
    "papermill": {
     "duration": 0.032934,
     "end_time": "2025-02-04T11:08:30.204506",
     "exception": false,
     "start_time": "2025-02-04T11:08:30.171572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13d534ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:30.268462Z",
     "iopub.status.busy": "2025-02-04T11:08:30.268103Z",
     "iopub.status.idle": "2025-02-04T11:08:30.272685Z",
     "shell.execute_reply": "2025-02-04T11:08:30.271938Z"
    },
    "papermill": {
     "duration": 0.03816,
     "end_time": "2025-02-04T11:08:30.274151",
     "exception": false,
     "start_time": "2025-02-04T11:08:30.235991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Define Dataloaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# # Define loss function and move model to device\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Function to train a single epoch\n",
    "# def train_epoch(model, loader, optimizer, scaler):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "    \n",
    "#     for images, labels in loader:\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         with torch.amp.autocast(device_type='cuda'):\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs.logits, labels)\n",
    "        \n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "        \n",
    "#         total_loss += loss.item() * images.size(0)\n",
    "#         preds = torch.argmax(outputs.logits, dim=1)\n",
    "#         correct += (preds == labels).sum().item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(loader.dataset)\n",
    "#     accuracy = correct / len(loader.dataset)\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "# # Function to validate the model\n",
    "# def validate(model, loader):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "            \n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "#             total_loss += loss.item() * images.size(0)\n",
    "#             preds = torch.argmax(outputs.logits, dim=1)\n",
    "#             correct += (preds == labels).sum().item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(loader.dataset)\n",
    "#     accuracy = correct / len(loader.dataset)\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "# scaler = torch.amp.GradScaler()\n",
    "# history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "# best_val_loss = float('inf')\n",
    "# best_model_state = None\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     train_loss, train_acc = train_epoch(model, train_loader, optimizer, scaler)\n",
    "#     val_loss, val_acc = validate(model, val_loader)\n",
    "    \n",
    "#     history['train_loss'].append(train_loss)\n",
    "#     history['val_loss'].append(val_loss)\n",
    "#     history['train_acc'].append(train_acc)\n",
    "#     history['val_acc'].append(val_acc)\n",
    "    \n",
    "#     print(f\"Epoch {epoch+1:02d}:\")\n",
    "#     print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
    "#     print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\\n\")\n",
    "    \n",
    "#     # Save the best model\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         best_model_state = model.state_dict()\n",
    "\n",
    "# # Load the best model at the end of training\n",
    "# best_model_path = \"/kaggle/working/best_vit_model.pth\"\n",
    "# if best_model_state:\n",
    "#     model.load_state_dict(best_model_state)\n",
    "#     torch.save(best_model_state, best_model_path)\n",
    "#     print(\"Best model loaded from training with lowest validation loss.\")\n",
    "\n",
    "# # Plot training history\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history['train_loss'], label='Train')\n",
    "# plt.plot(history['val_loss'], label='Validation')\n",
    "# plt.title('Loss Curve')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history['train_acc'], label='Train')\n",
    "# plt.plot(history['val_acc'], label='Validation')\n",
    "# plt.title('Accuracy Curve')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e441ff",
   "metadata": {
    "papermill": {
     "duration": 0.031832,
     "end_time": "2025-02-04T11:08:30.337923",
     "exception": false,
     "start_time": "2025-02-04T11:08:30.306091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8720c5e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:30.402934Z",
     "iopub.status.busy": "2025-02-04T11:08:30.402602Z",
     "iopub.status.idle": "2025-02-04T11:08:30.406545Z",
     "shell.execute_reply": "2025-02-04T11:08:30.405453Z"
    },
    "papermill": {
     "duration": 0.038344,
     "end_time": "2025-02-04T11:08:30.408259",
     "exception": false,
     "start_time": "2025-02-04T11:08:30.369915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_dir = \"/kaggle/working/vit_model\"\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# torch.save(model.state_dict(), os.path.join(save_dir, \"vit_model_weights.pth\"))\n",
    "\n",
    "# processor.save_pretrained(save_dir)\n",
    "\n",
    "# class_info = {\n",
    "#     \"class_order\": submission_class_order,\n",
    "#     \"class_to_idx\": train_dataset.class_to_idx,\n",
    "#     \"idx_to_class\": {v: k for k, v in train_dataset.class_to_idx.items()}\n",
    "# }\n",
    "\n",
    "# torch.save(class_info, os.path.join(save_dir, \"class_info.pth\"))\n",
    "\n",
    "# train_config = {\n",
    "#     \"epochs_trained\": len(history['train_loss']),\n",
    "#     \"best_val_acc\": max(history['val_acc']),\n",
    "#     \"optimizer_state\": optimizer.state_dict()\n",
    "# }\n",
    "\n",
    "# torch.save(train_config, os.path.join(save_dir, \"train_config.pth\"))\n",
    "\n",
    "# print(f\"Model Saved to：{save_dir}\")\n",
    "\n",
    "# # Second Hugging Face Format Save\n",
    "# model.save_pretrained(save_dir)\n",
    "# processor.save_pretrained(save_dir)\n",
    "\n",
    "# with open(os.path.join(save_dir, \"class_info.txt\"), \"w\") as f:\n",
    "#     f.write(\"\\n\".join(submission_class_order))\n",
    "\n",
    "# print(\"HuggingFace Format Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1f1f1",
   "metadata": {
    "papermill": {
     "duration": 0.03204,
     "end_time": "2025-02-04T11:08:30.473005",
     "exception": false,
     "start_time": "2025-02-04T11:08:30.440965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test and Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76a7052d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:30.537693Z",
     "iopub.status.busy": "2025-02-04T11:08:30.537353Z",
     "iopub.status.idle": "2025-02-04T11:08:30.541937Z",
     "shell.execute_reply": "2025-02-04T11:08:30.541063Z"
    },
    "papermill": {
     "duration": 0.038928,
     "end_time": "2025-02-04T11:08:30.543620",
     "exception": false,
     "start_time": "2025-02-04T11:08:30.504692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Set device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Define class order for submission\n",
    "# submission_class_order = [\n",
    "#     'Groove_billed_Ani', 'Red_winged_Blackbird', 'Rusty_Blackbird', 'Gray_Catbird',\n",
    "#     'Brandt_Cormorant', 'Eastern_Towhee', 'Indigo_Bunting', 'Brewer_Blackbird',\n",
    "#     'Painted_Bunting', 'Bobolink', 'Lazuli_Bunting', 'Yellow_headed_Blackbird',\n",
    "#     'American_Crow', 'Fish_Crow', 'Brown_Creeper', 'Yellow_billed_Cuckoo',\n",
    "#     'Yellow_breasted_Chat', 'Black_billed_Cuckoo', 'Gray_crowned_Rosy_Finch', 'Bronzed_Cowbird'\n",
    "# ]\n",
    "\n",
    "# # Image transformation for validation\n",
    "# processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "# ])\n",
    "\n",
    "# # Custom Dataset class for test images\n",
    "# class CompetitionTestDataset(Dataset):\n",
    "#     def __init__(self, test_dir, transform=None):\n",
    "#         self.test_dir = test_dir\n",
    "#         self.image_files = sorted(os.listdir(test_dir))\n",
    "#         self.image_paths = [os.path.join(test_dir, f) for f in self.image_files]\n",
    "#         self.transform = transform\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image, os.path.basename(self.image_paths[idx])\n",
    "\n",
    "# # Load best saved model\n",
    "# def load_best_model(model, model_path):\n",
    "#     model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "#     model.eval()\n",
    "#     return model\n",
    "\n",
    "# # Generate predictions using the best model\n",
    "# def generate_submission(test_dir, best_model_path, output_csv=\"submission.csv\"):\n",
    "#     test_dataset = CompetitionTestDataset(test_dir, transform=val_transform)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "    \n",
    "#     config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=len(submission_class_order))\n",
    "#     model = ViTForImageClassification(config).to(device)\n",
    "#     model = load_best_model(model, best_model_path)\n",
    "    \n",
    "#     filenames = []\n",
    "#     predictions = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for images, paths in test_loader:\n",
    "#             outputs = model(images.to(device))\n",
    "#             batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            \n",
    "#             filenames.extend(paths)\n",
    "#             predictions.extend(batch_preds.tolist())\n",
    "    \n",
    "#     submission_df = pd.DataFrame({\n",
    "#         'path': filenames,\n",
    "#         'class_idx': predictions\n",
    "#     })\n",
    "    \n",
    "#     print(\"\\nValidation Results:\")\n",
    "#     print(f\"Total Samples: {len(submission_df)}\")\n",
    "#     print(f\"Number of unique file names: {submission_df['path'].nunique()}\")\n",
    "#     print(f\"Predicted category distribution:\\n{submission_df['class_idx'].value_counts().sort_index()}\")\n",
    "    \n",
    "#     submission_df.to_csv(output_csv, index=False)\n",
    "#     print(f\"\\nSubmission saved to: {output_csv}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_dir = \"/kaggle/working/processed_images/test_images\"\n",
    "#     best_model_path = \"/kaggle/working/best_vit_model.pth\"  # Use best saved model\n",
    "    \n",
    "#     generate_submission(test_dir, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c2a2081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T11:08:30.608537Z",
     "iopub.status.busy": "2025-02-04T11:08:30.608212Z",
     "iopub.status.idle": "2025-02-04T11:08:30.611778Z",
     "shell.execute_reply": "2025-02-04T11:08:30.610768Z"
    },
    "papermill": {
     "duration": 0.037295,
     "end_time": "2025-02-04T11:08:30.613321",
     "exception": false,
     "start_time": "2025-02-04T11:08:30.576026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# shutil.rmtree('/kaggle/working/preprocessed02_images', ignore_errors=True)\n",
    "# shutil.rmtree('/kaggle/working/preprocessed_images', ignore_errors=True) \n",
    "# shutil.rmtree('/kaggle/working/processed_images', ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10794348,
     "sourceId": 91120,
     "sourceType": "competition"
    },
    {
     "datasetId": 6597767,
     "sourceId": 10655411,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1952.741288,
   "end_time": "2025-02-04T11:08:34.308031",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-04T10:36:01.566743",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
