{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed1e48e",
   "metadata": {
    "papermill": {
     "duration": 0.006436,
     "end_time": "2025-02-03T20:02:09.703082",
     "exception": false,
     "start_time": "2025-02-03T20:02:09.696646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332c909b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:02:09.716059Z",
     "iopub.status.busy": "2025-02-03T20:02:09.715551Z",
     "iopub.status.idle": "2025-02-03T20:02:19.776710Z",
     "shell.execute_reply": "2025-02-03T20:02:19.775185Z"
    },
    "papermill": {
     "duration": 10.070104,
     "end_time": "2025-02-03T20:02:19.778987",
     "exception": false,
     "start_time": "2025-02-03T20:02:09.708883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install transformers timm albumentations --quiet\n",
    "! pip install ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37bf3e34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:02:19.793699Z",
     "iopub.status.busy": "2025-02-03T20:02:19.793298Z",
     "iopub.status.idle": "2025-02-03T20:02:51.376373Z",
     "shell.execute_reply": "2025-02-03T20:02:51.375255Z"
    },
    "papermill": {
     "duration": 31.593409,
     "end_time": "2025-02-03T20:02:51.378412",
     "exception": false,
     "start_time": "2025-02-03T20:02:19.785003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Numerical and Data Handling Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image Processing Libraries\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Plotting and Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch and Torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Transformers for Vision Models\n",
    "from transformers import ViTConfig, ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "# Utility Libraries\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# jupyter nbextension enable --py widgetsnbextension\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c217ffcb",
   "metadata": {
    "papermill": {
     "duration": 0.005457,
     "end_time": "2025-02-03T20:02:51.389916",
     "exception": false,
     "start_time": "2025-02-03T20:02:51.384459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing & Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61fdb9e",
   "metadata": {
    "papermill": {
     "duration": 0.005331,
     "end_time": "2025-02-03T20:02:51.400904",
     "exception": false,
     "start_time": "2025-02-03T20:02:51.395573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Split the Dataset by 80 / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44586fa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:02:51.414088Z",
     "iopub.status.busy": "2025-02-03T20:02:51.413400Z",
     "iopub.status.idle": "2025-02-03T20:03:03.022280Z",
     "shell.execute_reply": "2025-02-03T20:03:03.020503Z"
    },
    "papermill": {
     "duration": 11.617861,
     "end_time": "2025-02-03T20:03:03.024401",
     "exception": false,
     "start_time": "2025-02-03T20:02:51.406540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset reorganization complete. New train and validation sets are saved in /kaggle/working/New_Split_Dataset.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Path to the dataset\n",
    "base_dir = '/kaggle/input/bdma-07-competition/BDMA7_project_files'\n",
    "train_dir = os.path.join(base_dir, 'train_images')\n",
    "val_dir = os.path.join(base_dir, 'val_images')\n",
    "output_base_dir = '/kaggle/working/New_Split_Dataset'  # Kaggle working directory for output\n",
    "new_train_dir = os.path.join(output_base_dir, 'train_images')\n",
    "new_val_dir = os.path.join(output_base_dir, 'val_images')\n",
    "\n",
    "# Create new directories for reorganized data\n",
    "os.makedirs(new_train_dir, exist_ok=True)\n",
    "os.makedirs(new_val_dir, exist_ok=True)\n",
    "\n",
    "# Reorganize each class\n",
    "for class_name in os.listdir(train_dir):\n",
    "    class_train_path = os.path.join(train_dir, class_name)\n",
    "    class_val_path = os.path.join(val_dir, class_name)\n",
    "    \n",
    "    # Combine train and validation images for splitting\n",
    "    all_images = []\n",
    "    if os.path.isdir(class_train_path):\n",
    "        all_images.extend([os.path.join(class_train_path, img) for img in os.listdir(class_train_path)])\n",
    "    if os.path.isdir(class_val_path):\n",
    "        all_images.extend([os.path.join(class_val_path, img) for img in os.listdir(class_val_path)])\n",
    "    \n",
    "    # Split into 80% train, 20% validation\n",
    "    train_images, val_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create class directories in new_train_dir and new_val_dir\n",
    "    new_class_train_path = os.path.join(new_train_dir, class_name)\n",
    "    new_class_val_path = os.path.join(new_val_dir, class_name)\n",
    "    os.makedirs(new_class_train_path, exist_ok=True)\n",
    "    os.makedirs(new_class_val_path, exist_ok=True)\n",
    "    \n",
    "    # Move images to respective directories\n",
    "    for img_path in train_images:\n",
    "        shutil.copy(img_path, new_class_train_path)\n",
    "    for img_path in val_images:\n",
    "        shutil.copy(img_path, new_class_val_path)\n",
    "\n",
    "print(f\"Dataset reorganization complete. New train and validation sets are saved in {output_base_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d9ab2",
   "metadata": {
    "papermill": {
     "duration": 0.005649,
     "end_time": "2025-02-03T20:03:03.036378",
     "exception": false,
     "start_time": "2025-02-03T20:03:03.030729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1d9f8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:03.049575Z",
     "iopub.status.busy": "2025-02-03T20:03:03.049193Z",
     "iopub.status.idle": "2025-02-03T20:03:03.054207Z",
     "shell.execute_reply": "2025-02-03T20:03:03.052813Z"
    },
    "papermill": {
     "duration": 0.014016,
     "end_time": "2025-02-03T20:03:03.056193",
     "exception": false,
     "start_time": "2025-02-03T20:03:03.042177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Data augmentation (rotation)\n",
    "# train_dir = \"/kaggle/working/processed_images/train_images\"\n",
    "# output_dir = \"/kaggle/working/preprocessed_images\"\n",
    "\n",
    "# # Image processing parameters\n",
    "# img_size = (128, 128)  # Resize dimensions\n",
    "# rotation_angles = [-15, -10, -5, 5, 10, 15]  # Rotation angles\n",
    "\n",
    "# # Create output directory\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Data augmentation and saving\n",
    "# def preprocess_and_augment_images(input_dir, output_dir, img_size, angles):\n",
    "#     \"\"\"\n",
    "#     Perform preprocessing (resize) and augmentation (rotation) on images.\n",
    "#     \"\"\"\n",
    "#     for class_folder in tqdm(os.listdir(input_dir), desc=\"Processing Classes\"):\n",
    "#         class_path = os.path.join(input_dir, class_folder)\n",
    "#         if not os.path.isdir(class_path):\n",
    "#             continue\n",
    "\n",
    "#         # Create output directory for each class\n",
    "#         class_output_path = os.path.join(output_dir, class_folder)\n",
    "#         os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "#         # Process each image\n",
    "#         for img_file in os.listdir(class_path):\n",
    "#             img_path = os.path.join(class_path, img_file)\n",
    "#             img = cv2.imread(img_path)\n",
    "\n",
    "#             if img is None:\n",
    "#                 print(f\"Warning: Unable to read image {img_path}\")\n",
    "#                 continue\n",
    "\n",
    "#             # Resize the image\n",
    "#             resized_img = cv2.resize(img, img_size)\n",
    "\n",
    "#             # Save the resized original image\n",
    "#             base_name = os.path.splitext(img_file)[0]\n",
    "#             cv2.imwrite(os.path.join(class_output_path, f\"{base_name}_original.jpg\"), resized_img)\n",
    "\n",
    "#             # Data augmentation (rotation)\n",
    "#             for angle in angles:\n",
    "#                 # Calculate the rotation matrix\n",
    "#                 h, w = resized_img.shape[:2]\n",
    "#                 center = (w // 2, h // 2)\n",
    "#                 rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "#                 # Rotate the image\n",
    "#                 rotated_img = cv2.warpAffine(resized_img, rotation_matrix, (w, h))\n",
    "\n",
    "#                 # Save the augmented image\n",
    "#                 augmented_name = f\"{base_name}_rotated_{angle}.jpg\"\n",
    "#                 cv2.imwrite(os.path.join(class_output_path, augmented_name), rotated_img)\n",
    "\n",
    "# # Execute preprocessing and augmentation\n",
    "# preprocess_and_augment_images(train_dir, output_dir, img_size, rotation_angles)\n",
    "\n",
    "# print(\"Data preprocessing and augmentation complete. Files saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d10c57f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:03.070374Z",
     "iopub.status.busy": "2025-02-03T20:03:03.069968Z",
     "iopub.status.idle": "2025-02-03T20:03:03.075001Z",
     "shell.execute_reply": "2025-02-03T20:03:03.073721Z"
    },
    "papermill": {
     "duration": 0.014393,
     "end_time": "2025-02-03T20:03:03.076908",
     "exception": false,
     "start_time": "2025-02-03T20:03:03.062515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Data augmentation (flip)\n",
    "# def flip_image(image, flip_code):\n",
    "#     return cv2.flip(image, flip_code)\n",
    "\n",
    "# train_dir = \"/kaggle/working/preprocessed_images\"\n",
    "# output_dir = \"/kaggle/working/preprocessed02_images\"\n",
    "# # Define a list of flip codes\n",
    "# # flip_codes = [0, 1, -1]\n",
    "# flip_codes = [1]\n",
    "\n",
    "# # Iterate through subdirectories (classes) in the input directory\n",
    "# for root, dirs, files in os.walk(train_dir):\n",
    "#     for dir_name in dirs:\n",
    "#         input_class_dir = os.path.join(root, dir_name)\n",
    "#         output_class_dir = os.path.join(output_dir, dir_name)\n",
    "#         # Create the output directory if it doesn't exist\n",
    "#         if not os.path.exists(output_class_dir):\n",
    "#             os.makedirs(output_class_dir)\n",
    "#         # Iterate through files in the class directory\n",
    "#         for file in os.listdir(input_class_dir):\n",
    "#             if file.endswith(\".jpg\"):\n",
    "#                 input_file_path = os.path.join(input_class_dir, file)\n",
    "#                 # Read the image\n",
    "#                 image = cv2.imread(input_file_path)\n",
    "                \n",
    "#                 # Copy the original image to the output directory\n",
    "#                 output_original_file_path = os.path.join(output_class_dir, file)\n",
    "#                 cv2.imwrite(output_original_file_path, image)\n",
    "                \n",
    "#                 # Generate flipped images for each flip operation\n",
    "#                 for flip_code in flip_codes:\n",
    "#                     # Generate a new file name\n",
    "#                     base_name = os.path.splitext(file)[0]\n",
    "#                     output_file_name = f\"{base_name}_flipped_{flip_code}.jpg\"\n",
    "#                     output_file_path = os.path.join(output_class_dir, output_file_name)\n",
    "#                     # Perform the flip operation on the image\n",
    "#                     flipped_image = flip_image(image, flip_code)\n",
    "#                     cv2.imwrite(output_file_path, flipped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4d7c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:03.090327Z",
     "iopub.status.busy": "2025-02-03T20:03:03.089873Z",
     "iopub.status.idle": "2025-02-03T20:03:19.954174Z",
     "shell.execute_reply": "2025-02-03T20:03:19.952943Z"
    },
    "papermill": {
     "duration": 16.872993,
     "end_time": "2025-02-03T20:03:19.955887",
     "exception": false,
     "start_time": "2025-02-03T20:03:03.082894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Classes: 100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Classes: 100%|██████████| 20/20 [00:01<00:00, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Augmented and preprocessed data saved to:\n",
      "Train: /kaggle/working/Augmented_Processed_Dataset/train_images\n",
      "Validation: /kaggle/working/Augmented_Processed_Dataset/val_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Paths to the new train and validation datasets\n",
    "train_dir = \"/kaggle/working/New_Split_Dataset/train_images\"\n",
    "val_dir = \"/kaggle/working/New_Split_Dataset/val_images\"\n",
    "augmented_train_dir = \"/kaggle/working/Augmented_Processed_Dataset/train_images\"\n",
    "augmented_val_dir = \"/kaggle/working/Augmented_Processed_Dataset/val_images\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(augmented_train_dir, exist_ok=True)\n",
    "os.makedirs(augmented_val_dir, exist_ok=True)\n",
    "\n",
    "# Define transformations for training and validation sets\n",
    "def get_train_augmentations():\n",
    "    return T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.RandomCrop((224, 224)),\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomRotation(degrees=15),\n",
    "        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        T.ToTensor(),  # Ensure Tensor Conversion\n",
    "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "def get_val_preprocessing():\n",
    "    return T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "# Function to process images and save results\n",
    "def process_images(input_dir, output_dir, transform, augment=False):\n",
    "    for class_folder in tqdm(os.listdir(input_dir), desc=\"Processing Classes\"):\n",
    "        class_path = os.path.join(input_dir, class_folder)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        # Create output directory for each class\n",
    "        class_output_path = os.path.join(output_dir, class_folder)\n",
    "        os.makedirs(class_output_path, exist_ok=True)\n",
    "        \n",
    "        # Process each image in the class directory\n",
    "        for img_file in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {img_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Apply transformations\n",
    "            transformed_img = transform(img)\n",
    "            base_name = os.path.splitext(img_file)[0]\n",
    "\n",
    "            if augment:\n",
    "                # Augmented images for training\n",
    "                output_file = os.path.join(class_output_path, f\"{base_name}_augmented.jpg\")\n",
    "            else:\n",
    "                # Preprocessed images for validation\n",
    "                output_file = os.path.join(class_output_path, f\"{base_name}.jpg\")\n",
    "            \n",
    "            # Convert back to PIL image and save\n",
    "            transformed_pil = T.ToPILImage()(transformed_img)\n",
    "            transformed_pil.save(output_file)\n",
    "\n",
    "# Process training set with data augmentation\n",
    "print(\"Augmenting training dataset...\")\n",
    "train_augmentations = get_train_augmentations()\n",
    "process_images(train_dir, augmented_train_dir, train_augmentations, augment=True)\n",
    "\n",
    "# Process validation set without augmentation\n",
    "print(\"Processing validation dataset...\")\n",
    "val_preprocessing = get_val_preprocessing()\n",
    "process_images(val_dir, augmented_val_dir, val_preprocessing, augment=False)\n",
    "\n",
    "print(\"Data processing complete. Augmented and preprocessed data saved to:\")\n",
    "print(f\"Train: {augmented_train_dir}\")\n",
    "print(f\"Validation: {augmented_val_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3740d63",
   "metadata": {
    "papermill": {
     "duration": 0.00793,
     "end_time": "2025-02-03T20:03:19.972293",
     "exception": false,
     "start_time": "2025-02-03T20:03:19.964363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocess by DeepLabV3 Remove Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce03ea8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:19.989655Z",
     "iopub.status.busy": "2025-02-03T20:03:19.989312Z",
     "iopub.status.idle": "2025-02-03T20:03:19.994378Z",
     "shell.execute_reply": "2025-02-03T20:03:19.993327Z"
    },
    "papermill": {
     "duration": 0.015889,
     "end_time": "2025-02-03T20:03:19.996119",
     "exception": false,
     "start_time": "2025-02-03T20:03:19.980230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torchvision.transforms as T\n",
    "# from PIL import Image\n",
    "# from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n",
    "# import numpy as np\n",
    "\n",
    "# # Set data paths\n",
    "# train_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/train_images\"\n",
    "# val_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/val_images\"\n",
    "# test_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/test_images/mistery_cat\"\n",
    "# output_dir = \"/kaggle/working/processed_images\"\n",
    "\n",
    "# # Create output directory\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Load the pretrained DeepLabV3+ model (ResNet101 backbone)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Load pretrained model with correct weights handling\n",
    "# weights = DeepLabV3_ResNet101_Weights.DEFAULT\n",
    "# model = deeplabv3_resnet101(weights=weights).to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # Image preprocessing (fixed \"mean\" and \"std\" issue)\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"Preprocess the input image for the DeepLabV3+ model.\"\"\"\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     transform = T.Compose([\n",
    "#         T.Resize((512, 512)),\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize(mean=weights.transforms().mean, std=weights.transforms().std),  # FIXED\n",
    "#     ])\n",
    "#     return transform(image).unsqueeze(0), image\n",
    "\n",
    "# # Post-processing: apply the segmentation mask to the original image\n",
    "# def apply_mask(image, mask):\n",
    "#     \"\"\"Apply the segmentation mask to the original image to remove the background.\"\"\"\n",
    "#     mask = mask.cpu().numpy()\n",
    "#     mask = (mask > 0.5).astype(np.uint8)  # Binarize the mask\n",
    "#     mask = Image.fromarray(mask * 255).resize(image.size, Image.BILINEAR)\n",
    "#     mask = np.array(mask) / 255\n",
    "#     image_np = np.array(image)\n",
    "#     result = (image_np * np.expand_dims(mask, axis=-1)).astype(np.uint8)\n",
    "#     return Image.fromarray(result)\n",
    "\n",
    "# def process_directory(input_dir, output_subdir):\n",
    "#     \"\"\"Process all images in a directory, including subdirectories, and save the results.\"\"\"\n",
    "#     output_path = os.path.join(output_dir, output_subdir)\n",
    "#     os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "#     print(f\"🔍 Removing images Background in: {input_dir}\")\n",
    "\n",
    "#     # Walk through all subdirectories and files\n",
    "#     for root, _, files in os.walk(input_dir):\n",
    "#         for filename in files:\n",
    "#             file_path = os.path.join(root, filename)\n",
    "\n",
    "#             # Process only image files\n",
    "#             if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "#                 # Recreate subdirectory structure in output\n",
    "#                 relative_path = os.path.relpath(root, input_dir)\n",
    "#                 sub_output_path = os.path.join(output_path, relative_path)\n",
    "#                 os.makedirs(sub_output_path, exist_ok=True)\n",
    "\n",
    "#                 try:\n",
    "#                     # Preprocess the image\n",
    "#                     input_tensor, original_image = preprocess_image(file_path)\n",
    "#                     input_tensor = input_tensor.to(device)\n",
    "\n",
    "#                     # Inference\n",
    "#                     with torch.no_grad():\n",
    "#                         output = model(input_tensor)['out']\n",
    "#                     mask = torch.argmax(output.squeeze(), dim=0)\n",
    "\n",
    "#                     # Apply the mask\n",
    "#                     result_image = apply_mask(original_image, mask)\n",
    "\n",
    "#                     # Save the result\n",
    "#                     result_path = os.path.join(sub_output_path, filename)\n",
    "#                     result_image.save(result_path)\n",
    "#                     # print(f\"✅ Processed and saved: {result_path}\")\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"❌ Error processing {file_path}: {e}\")  # Debugging output\n",
    "\n",
    "# # Process training, validation, and test datasets\n",
    "# print(\"Processing training images...\")\n",
    "# process_directory(train_dir, \"train_images\")\n",
    "\n",
    "# print(\"Processing validation images...\")\n",
    "# process_directory(val_dir, \"val_images\")\n",
    "\n",
    "# print(\"Processing test images...\")\n",
    "# process_directory(test_dir, \"test_images\")\n",
    "\n",
    "# print(f\"All images processed and saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c35e70b",
   "metadata": {
    "papermill": {
     "duration": 0.007679,
     "end_time": "2025-02-03T20:03:20.011916",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.004237",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Vision Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c85ff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:20.029304Z",
     "iopub.status.busy": "2025-02-03T20:03:20.028868Z",
     "iopub.status.idle": "2025-02-03T20:03:20.032788Z",
     "shell.execute_reply": "2025-02-03T20:03:20.031739Z"
    },
    "papermill": {
     "duration": 0.014528,
     "end_time": "2025-02-03T20:03:20.034538",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.020010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dir = \"/kaggle/working/preprocessed02_images\"\n",
    "# val_dir = \"/kaggle/working/processed_images/val_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49fa4cb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:20.052879Z",
     "iopub.status.busy": "2025-02-03T20:03:20.052491Z",
     "iopub.status.idle": "2025-02-03T20:03:20.056721Z",
     "shell.execute_reply": "2025-02-03T20:03:20.055496Z"
    },
    "papermill": {
     "duration": 0.016216,
     "end_time": "2025-02-03T20:03:20.058958",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.042742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # model_name = \"google/vit-base-patch16-224-in21k\"  # 224x224 Input，16x16 Chunk\n",
    "# # Change to ViT-Large\n",
    "# model_name = \"google/vit-large-patch16-224-in21k\"\n",
    "# processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "# model = ViTForImageClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=20,  # Number of Classes\n",
    "#     ignore_mismatched_sizes=True  \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050313e",
   "metadata": {
    "papermill": {
     "duration": 0.008384,
     "end_time": "2025-02-03T20:03:20.076200",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.067816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Freeze feature layers of ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38b97b9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:20.094477Z",
     "iopub.status.busy": "2025-02-03T20:03:20.094120Z",
     "iopub.status.idle": "2025-02-03T20:03:20.098046Z",
     "shell.execute_reply": "2025-02-03T20:03:20.096985Z"
    },
    "papermill": {
     "duration": 0.015008,
     "end_time": "2025-02-03T20:03:20.099821",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.084813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Freeze all feature layers of ViT (train only the classification head)\n",
    "# for param in model.vit.parameters():\n",
    "#     # param.requires_grad = False\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Thaw the classification headers separately\n",
    "# for param in model.classifier.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8154838b",
   "metadata": {
    "papermill": {
     "duration": 0.00792,
     "end_time": "2025-02-03T20:03:20.116154",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.108234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Set layer-wise learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "207018b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:20.133461Z",
     "iopub.status.busy": "2025-02-03T20:03:20.133125Z",
     "iopub.status.idle": "2025-02-03T20:03:20.137101Z",
     "shell.execute_reply": "2025-02-03T20:03:20.135979Z"
    },
    "papermill": {
     "duration": 0.014733,
     "end_time": "2025-02-03T20:03:20.138913",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.124180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Set layer-wise learning rate (feature layer learning rate is lower)\n",
    "# optimizer = AdamW(\n",
    "#     [\n",
    "#         {\"params\": model.vit.parameters(), \"lr\": 1e-5},  # Feature extraction layer\n",
    "#         {\"params\": model.classifier.parameters(), \"lr\": 3e-4}  # Classification Header\n",
    "#     ],\n",
    "#     weight_decay=0.01\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d618954f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:20.158449Z",
     "iopub.status.busy": "2025-02-03T20:03:20.157962Z",
     "iopub.status.idle": "2025-02-03T20:03:20.163052Z",
     "shell.execute_reply": "2025-02-03T20:03:20.161874Z"
    },
    "papermill": {
     "duration": 0.016705,
     "end_time": "2025-02-03T20:03:20.164989",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.148284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Verify that the classification layer is randomly initialized (correct state should be True)\n",
    "# print(model.classifier.weight.mean().item())  # Should be close to 0 (normal distribution initialization)\n",
    "# print(model.classifier.bias.mean().item())    # Should be close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcfcd5",
   "metadata": {
    "papermill": {
     "duration": 0.008277,
     "end_time": "2025-02-03T20:03:20.181751",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.173474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check Submission Classes Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "679221b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:20.199879Z",
     "iopub.status.busy": "2025-02-03T20:03:20.199518Z",
     "iopub.status.idle": "2025-02-03T20:03:20.203543Z",
     "shell.execute_reply": "2025-02-03T20:03:20.202577Z"
    },
    "papermill": {
     "duration": 0.015422,
     "end_time": "2025-02-03T20:03:20.205349",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.189927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission_class_order = [\n",
    "#     'Groove_billed_Ani',\n",
    "#     'Red_winged_Blackbird',\n",
    "#     'Rusty_Blackbird',\n",
    "#     'Gray_Catbird',\n",
    "#     'Brandt_Cormorant',\n",
    "#     'Eastern_Towhee',\n",
    "#     'Indigo_Bunting',\n",
    "#     'Brewer_Blackbird',\n",
    "#     'Painted_Bunting',\n",
    "#     'Bobolink',\n",
    "#     'Lazuli_Bunting',\n",
    "#     'Yellow_headed_Blackbird',\n",
    "#     'American_Crow',\n",
    "#     'Fish_Crow',\n",
    "#     'Brown_Creeper',\n",
    "#     'Yellow_billed_Cuckoo',\n",
    "#     'Yellow_breasted_Chat',\n",
    "#     'Black_billed_Cuckoo',\n",
    "#     'Gray_crowned_Rosy_Finch',\n",
    "#     'Bronzed_Cowbird'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03c7427f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:20.223501Z",
     "iopub.status.busy": "2025-02-03T20:03:20.223127Z",
     "iopub.status.idle": "2025-02-03T20:03:20.228084Z",
     "shell.execute_reply": "2025-02-03T20:03:20.227092Z"
    },
    "papermill": {
     "duration": 0.015944,
     "end_time": "2025-02-03T20:03:20.229891",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.213947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class BirdDataset(Dataset):\n",
    "#     def __init__(self, main_dir, transform=None):\n",
    "#         self.dataset = ImageFolder(root=main_dir, transform=transform)\n",
    "#         self.class_to_idx = self.dataset.class_to_idx\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         image, label = self.dataset[idx]\n",
    "#         return image, label\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.RandomCrop(224),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "# ])\n",
    "\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "# ])\n",
    "\n",
    "# train_dataset = BirdDataset(train_dir, transform=train_transform)\n",
    "# val_dataset = BirdDataset(val_dir, transform=val_transform)\n",
    "\n",
    "# def validate_class_order(train_class_order, submission_order):\n",
    "#     \"\"\"Make sure the category names and order of both lists are exactly the same\"\"\"\n",
    "#     if len(train_class_order) != len(submission_order):\n",
    "#         raise ValueError(f\"The number of categories does not match! Training set: {len(train_class_order)}, Submission Requirements: {len(submission_order)}\")\n",
    "    \n",
    "#     for train_cls, sub_cls in zip(train_class_order, submission_order):\n",
    "#         if train_cls != sub_cls:\n",
    "#             raise ValueError(f\"Inconsistent order: training set '{train_cls}' vs Submission Requirements '{sub_cls}'\")\n",
    "#     return True\n",
    "\n",
    "# train_class_order = sorted(train_dataset.class_to_idx.keys())\n",
    "\n",
    "# try:\n",
    "#     validate_class_order(train_class_order, submission_class_order)\n",
    "# except ValueError as e:\n",
    "#     print(\"Category order inconsistency detected, automatically correcting...\")\n",
    "#     from torchvision.datasets import DatasetFolder\n",
    "    \n",
    "#     class OrderedImageFolder(DatasetFolder):\n",
    "#         \"\"\"Forces the data sets of categories to be loaded in a specified order\"\"\"\n",
    "#         def __init__(self, root, class_order, transform=None):\n",
    "#             self.class_order = class_order\n",
    "#             super().__init__(\n",
    "#                 root,\n",
    "#                 loader=lambda x: Image.open(x).convert(\"RGB\"),\n",
    "#                 extensions=('jpg', 'jpeg', 'png'),\n",
    "#                 transform=transform,\n",
    "#                 target_transform=None\n",
    "#             )\n",
    "            \n",
    "#         def find_classes(self, directory):\n",
    "#             classes = self.class_order \n",
    "#             class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "#             return classes, class_to_idx\n",
    "    \n",
    "#     train_dataset = OrderedImageFolder(\n",
    "#         train_dir, \n",
    "#         class_order=submission_class_order,\n",
    "#         transform=train_transform\n",
    "#     )\n",
    "#     val_dataset = OrderedImageFolder(\n",
    "#         val_dir,\n",
    "#         class_order=submission_class_order,\n",
    "#         transform=val_transform\n",
    "#     )\n",
    "    \n",
    "#     print(\"Corrected category order：\", train_dataset.classes)\n",
    "    \n",
    "# # Category index validation (ensuring consistency with submission format)\n",
    "# assert sorted(train_dataset.class_to_idx.keys()) == sorted(submission_class_order), \"Category order mismatch！\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffb481",
   "metadata": {
    "papermill": {
     "duration": 0.008991,
     "end_time": "2025-02-03T20:03:20.247221",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.238230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee90a4cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:20.264845Z",
     "iopub.status.busy": "2025-02-03T20:03:20.264475Z",
     "iopub.status.idle": "2025-02-03T20:03:20.269743Z",
     "shell.execute_reply": "2025-02-03T20:03:20.268706Z"
    },
    "papermill": {
     "duration": 0.016155,
     "end_time": "2025-02-03T20:03:20.271488",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.255333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Define Dataloaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# # Define loss function and move model to device\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Function to train a single epoch\n",
    "# def train_epoch(model, loader, optimizer, scaler):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "    \n",
    "#     for images, labels in loader:\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         with torch.amp.autocast(device_type='cuda'):\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs.logits, labels)\n",
    "        \n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "        \n",
    "#         total_loss += loss.item() * images.size(0)\n",
    "#         preds = torch.argmax(outputs.logits, dim=1)\n",
    "#         correct += (preds == labels).sum().item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(loader.dataset)\n",
    "#     accuracy = correct / len(loader.dataset)\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "# # Function to validate the model\n",
    "# def validate(model, loader):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "            \n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "#             total_loss += loss.item() * images.size(0)\n",
    "#             preds = torch.argmax(outputs.logits, dim=1)\n",
    "#             correct += (preds == labels).sum().item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(loader.dataset)\n",
    "#     accuracy = correct / len(loader.dataset)\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "# scaler = torch.amp.GradScaler()\n",
    "# history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "# best_val_loss = float('inf')\n",
    "# best_model_state = None\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     train_loss, train_acc = train_epoch(model, train_loader, optimizer, scaler)\n",
    "#     val_loss, val_acc = validate(model, val_loader)\n",
    "    \n",
    "#     history['train_loss'].append(train_loss)\n",
    "#     history['val_loss'].append(val_loss)\n",
    "#     history['train_acc'].append(train_acc)\n",
    "#     history['val_acc'].append(val_acc)\n",
    "    \n",
    "#     print(f\"Epoch {epoch+1:02d}:\")\n",
    "#     print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
    "#     print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\\n\")\n",
    "    \n",
    "#     # Save the best model\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         best_model_state = model.state_dict()\n",
    "\n",
    "# # Load the best model at the end of training\n",
    "# best_model_path = \"/kaggle/working/best_vit_model.pth\"\n",
    "# if best_model_state:\n",
    "#     model.load_state_dict(best_model_state)\n",
    "#     torch.save(best_model_state, best_model_path)\n",
    "#     print(\"Best model loaded from training with lowest validation loss.\")\n",
    "\n",
    "# # Plot training history\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history['train_loss'], label='Train')\n",
    "# plt.plot(history['val_loss'], label='Validation')\n",
    "# plt.title('Loss Curve')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history['train_acc'], label='Train')\n",
    "# plt.plot(history['val_acc'], label='Validation')\n",
    "# plt.title('Accuracy Curve')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f42c4a4",
   "metadata": {
    "papermill": {
     "duration": 0.007795,
     "end_time": "2025-02-03T20:03:20.287796",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.280001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c8f5825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:20.308222Z",
     "iopub.status.busy": "2025-02-03T20:03:20.307783Z",
     "iopub.status.idle": "2025-02-03T20:03:20.313183Z",
     "shell.execute_reply": "2025-02-03T20:03:20.311682Z"
    },
    "papermill": {
     "duration": 0.019456,
     "end_time": "2025-02-03T20:03:20.315961",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.296505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_dir = \"/kaggle/working/vit_model\"\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# torch.save(model.state_dict(), os.path.join(save_dir, \"vit_model_weights.pth\"))\n",
    "\n",
    "# processor.save_pretrained(save_dir)\n",
    "\n",
    "# class_info = {\n",
    "#     \"class_order\": submission_class_order,\n",
    "#     \"class_to_idx\": train_dataset.class_to_idx,\n",
    "#     \"idx_to_class\": {v: k for k, v in train_dataset.class_to_idx.items()}\n",
    "# }\n",
    "\n",
    "# torch.save(class_info, os.path.join(save_dir, \"class_info.pth\"))\n",
    "\n",
    "# train_config = {\n",
    "#     \"epochs_trained\": len(history['train_loss']),\n",
    "#     \"best_val_acc\": max(history['val_acc']),\n",
    "#     \"optimizer_state\": optimizer.state_dict()\n",
    "# }\n",
    "\n",
    "# torch.save(train_config, os.path.join(save_dir, \"train_config.pth\"))\n",
    "\n",
    "# print(f\"Model Saved to：{save_dir}\")\n",
    "\n",
    "# # Second Hugging Face Format Save\n",
    "# model.save_pretrained(save_dir)\n",
    "# processor.save_pretrained(save_dir)\n",
    "\n",
    "# with open(os.path.join(save_dir, \"class_info.txt\"), \"w\") as f:\n",
    "#     f.write(\"\\n\".join(submission_class_order))\n",
    "\n",
    "# print(\"HuggingFace Format Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f14fa",
   "metadata": {
    "papermill": {
     "duration": 0.008121,
     "end_time": "2025-02-03T20:03:20.334170",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.326049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test and Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b998141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:20.353111Z",
     "iopub.status.busy": "2025-02-03T20:03:20.352608Z",
     "iopub.status.idle": "2025-02-03T20:03:20.357598Z",
     "shell.execute_reply": "2025-02-03T20:03:20.356605Z"
    },
    "papermill": {
     "duration": 0.016423,
     "end_time": "2025-02-03T20:03:20.359445",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.343022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Set device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Define class order for submission\n",
    "# submission_class_order = [\n",
    "#     'Groove_billed_Ani', 'Red_winged_Blackbird', 'Rusty_Blackbird', 'Gray_Catbird',\n",
    "#     'Brandt_Cormorant', 'Eastern_Towhee', 'Indigo_Bunting', 'Brewer_Blackbird',\n",
    "#     'Painted_Bunting', 'Bobolink', 'Lazuli_Bunting', 'Yellow_headed_Blackbird',\n",
    "#     'American_Crow', 'Fish_Crow', 'Brown_Creeper', 'Yellow_billed_Cuckoo',\n",
    "#     'Yellow_breasted_Chat', 'Black_billed_Cuckoo', 'Gray_crowned_Rosy_Finch', 'Bronzed_Cowbird'\n",
    "# ]\n",
    "\n",
    "# # Image transformation for validation\n",
    "# processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "# ])\n",
    "\n",
    "# # Custom Dataset class for test images\n",
    "# class CompetitionTestDataset(Dataset):\n",
    "#     def __init__(self, test_dir, transform=None):\n",
    "#         self.test_dir = test_dir\n",
    "#         self.image_files = sorted(os.listdir(test_dir))\n",
    "#         self.image_paths = [os.path.join(test_dir, f) for f in self.image_files]\n",
    "#         self.transform = transform\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image, os.path.basename(self.image_paths[idx])\n",
    "\n",
    "# # Load best saved model\n",
    "# def load_best_model(model, model_path):\n",
    "#     model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "#     model.eval()\n",
    "#     return model\n",
    "\n",
    "# # Generate predictions using the best model\n",
    "# def generate_submission(test_dir, best_model_path, output_csv=\"submission.csv\"):\n",
    "#     test_dataset = CompetitionTestDataset(test_dir, transform=val_transform)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "    \n",
    "#     config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=len(submission_class_order))\n",
    "#     model = ViTForImageClassification(config).to(device)\n",
    "#     model = load_best_model(model, best_model_path)\n",
    "    \n",
    "#     filenames = []\n",
    "#     predictions = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for images, paths in test_loader:\n",
    "#             outputs = model(images.to(device))\n",
    "#             batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            \n",
    "#             filenames.extend(paths)\n",
    "#             predictions.extend(batch_preds.tolist())\n",
    "    \n",
    "#     submission_df = pd.DataFrame({\n",
    "#         'path': filenames,\n",
    "#         'class_idx': predictions\n",
    "#     })\n",
    "    \n",
    "#     print(\"\\nValidation Results:\")\n",
    "#     print(f\"Total Samples: {len(submission_df)}\")\n",
    "#     print(f\"Number of unique file names: {submission_df['path'].nunique()}\")\n",
    "#     print(f\"Predicted category distribution:\\n{submission_df['class_idx'].value_counts().sort_index()}\")\n",
    "    \n",
    "#     submission_df.to_csv(output_csv, index=False)\n",
    "#     print(f\"\\nSubmission saved to: {output_csv}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_dir = \"/kaggle/working/processed_images/test_images\"\n",
    "#     best_model_path = \"/kaggle/working/best_vit_model.pth\"  # Use best saved model\n",
    "    \n",
    "#     generate_submission(test_dir, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72b4e7a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:03:20.377881Z",
     "iopub.status.busy": "2025-02-03T20:03:20.377520Z",
     "iopub.status.idle": "2025-02-03T20:03:20.381490Z",
     "shell.execute_reply": "2025-02-03T20:03:20.380433Z"
    },
    "papermill": {
     "duration": 0.015239,
     "end_time": "2025-02-03T20:03:20.383493",
     "exception": false,
     "start_time": "2025-02-03T20:03:20.368254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# shutil.rmtree('/kaggle/working/preprocessed02_images', ignore_errors=True)\n",
    "# shutil.rmtree('/kaggle/working/preprocessed_images', ignore_errors=True) \n",
    "# shutil.rmtree('/kaggle/working/processed_images', ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10794348,
     "sourceId": 91120,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 76.645113,
   "end_time": "2025-02-03T20:03:23.472191",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-03T20:02:06.827078",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
