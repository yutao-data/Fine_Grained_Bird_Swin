{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "913189c0",
   "metadata": {
    "papermill": {
     "duration": 0.008364,
     "end_time": "2025-02-03T20:21:20.148940",
     "exception": false,
     "start_time": "2025-02-03T20:21:20.140576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5695e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:21:20.163227Z",
     "iopub.status.busy": "2025-02-03T20:21:20.162803Z",
     "iopub.status.idle": "2025-02-03T20:21:29.856641Z",
     "shell.execute_reply": "2025-02-03T20:21:29.855107Z"
    },
    "papermill": {
     "duration": 9.703114,
     "end_time": "2025-02-03T20:21:29.858533",
     "exception": false,
     "start_time": "2025-02-03T20:21:20.155419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install transformers timm albumentations --quiet\n",
    "! pip install ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b664880c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:21:29.873033Z",
     "iopub.status.busy": "2025-02-03T20:21:29.872674Z",
     "iopub.status.idle": "2025-02-03T20:22:00.765406Z",
     "shell.execute_reply": "2025-02-03T20:22:00.764265Z"
    },
    "papermill": {
     "duration": 30.90232,
     "end_time": "2025-02-03T20:22:00.767453",
     "exception": false,
     "start_time": "2025-02-03T20:21:29.865133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Numerical and Data Handling Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image Processing Libraries\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Plotting and Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch and Torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Transformers for Vision Models\n",
    "from transformers import ViTConfig, ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "# Utility Libraries\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# jupyter nbextension enable --py widgetsnbextension\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886cc92b",
   "metadata": {
    "papermill": {
     "duration": 0.006131,
     "end_time": "2025-02-03T20:22:00.780342",
     "exception": false,
     "start_time": "2025-02-03T20:22:00.774211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing & Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688699d5",
   "metadata": {
    "papermill": {
     "duration": 0.006017,
     "end_time": "2025-02-03T20:22:00.792720",
     "exception": false,
     "start_time": "2025-02-03T20:22:00.786703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Split the Dataset by 80 / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402500ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:00.807309Z",
     "iopub.status.busy": "2025-02-03T20:22:00.806612Z",
     "iopub.status.idle": "2025-02-03T20:22:07.224040Z",
     "shell.execute_reply": "2025-02-03T20:22:07.222635Z"
    },
    "papermill": {
     "duration": 6.426981,
     "end_time": "2025-02-03T20:22:07.225984",
     "exception": false,
     "start_time": "2025-02-03T20:22:00.799003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset reorganization complete. New train and validation sets are saved in /kaggle/working/New_Split_Dataset.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Path to the dataset\n",
    "base_dir = '/kaggle/input/bdma-07-competition/BDMA7_project_files'\n",
    "train_dir = os.path.join(base_dir, 'train_images')\n",
    "val_dir = os.path.join(base_dir, 'val_images')\n",
    "output_base_dir = '/kaggle/working/New_Split_Dataset'  # Kaggle working directory for output\n",
    "new_train_dir = os.path.join(output_base_dir, 'train_images')\n",
    "new_val_dir = os.path.join(output_base_dir, 'val_images')\n",
    "\n",
    "# Create new directories for reorganized data\n",
    "os.makedirs(new_train_dir, exist_ok=True)\n",
    "os.makedirs(new_val_dir, exist_ok=True)\n",
    "\n",
    "# Reorganize each class\n",
    "for class_name in os.listdir(train_dir):\n",
    "    class_train_path = os.path.join(train_dir, class_name)\n",
    "    class_val_path = os.path.join(val_dir, class_name)\n",
    "    \n",
    "    # Combine train and validation images for splitting\n",
    "    all_images = []\n",
    "    if os.path.isdir(class_train_path):\n",
    "        all_images.extend([os.path.join(class_train_path, img) for img in os.listdir(class_train_path)])\n",
    "    if os.path.isdir(class_val_path):\n",
    "        all_images.extend([os.path.join(class_val_path, img) for img in os.listdir(class_val_path)])\n",
    "    \n",
    "    # Split into 80% train, 20% validation\n",
    "    train_images, val_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create class directories in new_train_dir and new_val_dir\n",
    "    new_class_train_path = os.path.join(new_train_dir, class_name)\n",
    "    new_class_val_path = os.path.join(new_val_dir, class_name)\n",
    "    os.makedirs(new_class_train_path, exist_ok=True)\n",
    "    os.makedirs(new_class_val_path, exist_ok=True)\n",
    "    \n",
    "    # Move images to respective directories\n",
    "    for img_path in train_images:\n",
    "        shutil.copy(img_path, new_class_train_path)\n",
    "    for img_path in val_images:\n",
    "        shutil.copy(img_path, new_class_val_path)\n",
    "\n",
    "print(f\"Dataset reorganization complete. New train and validation sets are saved in {output_base_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20e48b",
   "metadata": {
    "papermill": {
     "duration": 0.006088,
     "end_time": "2025-02-03T20:22:07.238644",
     "exception": false,
     "start_time": "2025-02-03T20:22:07.232556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b5a723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:07.252742Z",
     "iopub.status.busy": "2025-02-03T20:22:07.252302Z",
     "iopub.status.idle": "2025-02-03T20:22:07.257227Z",
     "shell.execute_reply": "2025-02-03T20:22:07.256057Z"
    },
    "papermill": {
     "duration": 0.013936,
     "end_time": "2025-02-03T20:22:07.258884",
     "exception": false,
     "start_time": "2025-02-03T20:22:07.244948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Data augmentation (rotation)\n",
    "# train_dir = \"/kaggle/working/processed_images/train_images\"\n",
    "# output_dir = \"/kaggle/working/preprocessed_images\"\n",
    "\n",
    "# # Image processing parameters\n",
    "# img_size = (128, 128)  # Resize dimensions\n",
    "# rotation_angles = [-15, -10, -5, 5, 10, 15]  # Rotation angles\n",
    "\n",
    "# # Create output directory\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Data augmentation and saving\n",
    "# def preprocess_and_augment_images(input_dir, output_dir, img_size, angles):\n",
    "#     \"\"\"\n",
    "#     Perform preprocessing (resize) and augmentation (rotation) on images.\n",
    "#     \"\"\"\n",
    "#     for class_folder in tqdm(os.listdir(input_dir), desc=\"Processing Classes\"):\n",
    "#         class_path = os.path.join(input_dir, class_folder)\n",
    "#         if not os.path.isdir(class_path):\n",
    "#             continue\n",
    "\n",
    "#         # Create output directory for each class\n",
    "#         class_output_path = os.path.join(output_dir, class_folder)\n",
    "#         os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "#         # Process each image\n",
    "#         for img_file in os.listdir(class_path):\n",
    "#             img_path = os.path.join(class_path, img_file)\n",
    "#             img = cv2.imread(img_path)\n",
    "\n",
    "#             if img is None:\n",
    "#                 print(f\"Warning: Unable to read image {img_path}\")\n",
    "#                 continue\n",
    "\n",
    "#             # Resize the image\n",
    "#             resized_img = cv2.resize(img, img_size)\n",
    "\n",
    "#             # Save the resized original image\n",
    "#             base_name = os.path.splitext(img_file)[0]\n",
    "#             cv2.imwrite(os.path.join(class_output_path, f\"{base_name}_original.jpg\"), resized_img)\n",
    "\n",
    "#             # Data augmentation (rotation)\n",
    "#             for angle in angles:\n",
    "#                 # Calculate the rotation matrix\n",
    "#                 h, w = resized_img.shape[:2]\n",
    "#                 center = (w // 2, h // 2)\n",
    "#                 rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "#                 # Rotate the image\n",
    "#                 rotated_img = cv2.warpAffine(resized_img, rotation_matrix, (w, h))\n",
    "\n",
    "#                 # Save the augmented image\n",
    "#                 augmented_name = f\"{base_name}_rotated_{angle}.jpg\"\n",
    "#                 cv2.imwrite(os.path.join(class_output_path, augmented_name), rotated_img)\n",
    "\n",
    "# # Execute preprocessing and augmentation\n",
    "# preprocess_and_augment_images(train_dir, output_dir, img_size, rotation_angles)\n",
    "\n",
    "# print(\"Data preprocessing and augmentation complete. Files saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807b4cde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:07.273378Z",
     "iopub.status.busy": "2025-02-03T20:22:07.272966Z",
     "iopub.status.idle": "2025-02-03T20:22:07.277680Z",
     "shell.execute_reply": "2025-02-03T20:22:07.276465Z"
    },
    "papermill": {
     "duration": 0.014169,
     "end_time": "2025-02-03T20:22:07.279573",
     "exception": false,
     "start_time": "2025-02-03T20:22:07.265404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Data augmentation (flip)\n",
    "# def flip_image(image, flip_code):\n",
    "#     return cv2.flip(image, flip_code)\n",
    "\n",
    "# train_dir = \"/kaggle/working/preprocessed_images\"\n",
    "# output_dir = \"/kaggle/working/preprocessed02_images\"\n",
    "# # Define a list of flip codes\n",
    "# # flip_codes = [0, 1, -1]\n",
    "# flip_codes = [1]\n",
    "\n",
    "# # Iterate through subdirectories (classes) in the input directory\n",
    "# for root, dirs, files in os.walk(train_dir):\n",
    "#     for dir_name in dirs:\n",
    "#         input_class_dir = os.path.join(root, dir_name)\n",
    "#         output_class_dir = os.path.join(output_dir, dir_name)\n",
    "#         # Create the output directory if it doesn't exist\n",
    "#         if not os.path.exists(output_class_dir):\n",
    "#             os.makedirs(output_class_dir)\n",
    "#         # Iterate through files in the class directory\n",
    "#         for file in os.listdir(input_class_dir):\n",
    "#             if file.endswith(\".jpg\"):\n",
    "#                 input_file_path = os.path.join(input_class_dir, file)\n",
    "#                 # Read the image\n",
    "#                 image = cv2.imread(input_file_path)\n",
    "                \n",
    "#                 # Copy the original image to the output directory\n",
    "#                 output_original_file_path = os.path.join(output_class_dir, file)\n",
    "#                 cv2.imwrite(output_original_file_path, image)\n",
    "                \n",
    "#                 # Generate flipped images for each flip operation\n",
    "#                 for flip_code in flip_codes:\n",
    "#                     # Generate a new file name\n",
    "#                     base_name = os.path.splitext(file)[0]\n",
    "#                     output_file_name = f\"{base_name}_flipped_{flip_code}.jpg\"\n",
    "#                     output_file_path = os.path.join(output_class_dir, output_file_name)\n",
    "#                     # Perform the flip operation on the image\n",
    "#                     flipped_image = flip_image(image, flip_code)\n",
    "#                     cv2.imwrite(output_file_path, flipped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6db041da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:07.294563Z",
     "iopub.status.busy": "2025-02-03T20:22:07.294100Z",
     "iopub.status.idle": "2025-02-03T20:22:28.458529Z",
     "shell.execute_reply": "2025-02-03T20:22:28.457233Z"
    },
    "papermill": {
     "duration": 21.173587,
     "end_time": "2025-02-03T20:22:28.460229",
     "exception": false,
     "start_time": "2025-02-03T20:22:07.286642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train_images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:14<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val_images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Augmented and preprocessed data saved to:\n",
      "Train: /kaggle/working/Augmented_Processed_Dataset/train_images\n",
      "Validation: /kaggle/working/Augmented_Processed_Dataset/val_images\n",
      "Test: /kaggle/working/Augmented_Processed_Dataset/test_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Paths to datasets\n",
    "train_dir = \"/kaggle/working/New_Split_Dataset/train_images\"\n",
    "val_dir = \"/kaggle/working/New_Split_Dataset/val_images\"\n",
    "test_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/test_images\"\n",
    "augmented_train_dir = \"/kaggle/working/Augmented_Processed_Dataset/train_images\"\n",
    "augmented_val_dir = \"/kaggle/working/Augmented_Processed_Dataset/val_images\"\n",
    "augmented_test_dir = \"/kaggle/working/Augmented_Processed_Dataset/test_images\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(augmented_train_dir, exist_ok=True)\n",
    "os.makedirs(augmented_val_dir, exist_ok=True)\n",
    "os.makedirs(augmented_test_dir, exist_ok=True)\n",
    "\n",
    "# Define transformations for training, validation, and test sets\n",
    "def get_train_augmentations():\n",
    "    return T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.RandomCrop((224, 224)),\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomRotation(degrees=15),\n",
    "        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        T.ToTensor(),  # Convert to Tensor\n",
    "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize\n",
    "    ])\n",
    "\n",
    "def get_val_preprocessing():\n",
    "    return T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize\n",
    "    ])\n",
    "\n",
    "# De-normalization function to convert tensors back to human-readable format\n",
    "def denormalize(tensor, mean, std):\n",
    "    \"\"\"\n",
    "    Reverse the normalization process to bring pixel values back to [0, 1].\n",
    "    \"\"\"\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Denormalize: t = t * std + mean\n",
    "    return tensor\n",
    "\n",
    "# Function to process images and save results\n",
    "def process_images(input_dir, output_dir, transform, augment=False):\n",
    "    for class_folder in tqdm(os.listdir(input_dir), desc=f\"Processing {os.path.basename(input_dir)}\"):\n",
    "        class_path = os.path.join(input_dir, class_folder)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        # Create output directory for each class\n",
    "        class_output_path = os.path.join(output_dir, class_folder)\n",
    "        os.makedirs(class_output_path, exist_ok=True)\n",
    "        \n",
    "        # Process each image in the class directory\n",
    "        for img_file in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {img_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Apply transformations\n",
    "            transformed_img = transform(img)\n",
    "            base_name = os.path.splitext(img_file)[0]\n",
    "\n",
    "            if augment:\n",
    "                # Augmented images for training\n",
    "                output_file = os.path.join(class_output_path, f\"{base_name}_augmented.jpg\")\n",
    "            else:\n",
    "                # Preprocessed images for validation and test\n",
    "                output_file = os.path.join(class_output_path, f\"{base_name}.jpg\")\n",
    "            \n",
    "            # Denormalize before saving\n",
    "            denormalized_img = denormalize(transformed_img.clone(), mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "            denormalized_img = denormalized_img.clamp(0, 1)  # Ensure pixel values are within [0, 1]\n",
    "\n",
    "            # Convert back to PIL image and save\n",
    "            transformed_pil = T.ToPILImage()(denormalized_img)\n",
    "            transformed_pil.save(output_file)\n",
    "\n",
    "# Process training set with data augmentation\n",
    "print(\"Augmenting training dataset...\")\n",
    "train_augmentations = get_train_augmentations()\n",
    "process_images(train_dir, augmented_train_dir, train_augmentations, augment=True)\n",
    "\n",
    "# Process validation set without augmentation\n",
    "print(\"Processing validation dataset...\")\n",
    "val_preprocessing = get_val_preprocessing()\n",
    "process_images(val_dir, augmented_val_dir, val_preprocessing, augment=False)\n",
    "\n",
    "# Process test set without augmentation\n",
    "print(\"Processing test dataset...\")\n",
    "process_images(test_dir, augmented_test_dir, val_preprocessing, augment=False)\n",
    "\n",
    "print(\"Data processing complete. Augmented and preprocessed data saved to:\")\n",
    "print(f\"Train: {augmented_train_dir}\")\n",
    "print(f\"Validation: {augmented_val_dir}\")\n",
    "print(f\"Test: {augmented_test_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ba11b",
   "metadata": {
    "papermill": {
     "duration": 0.008843,
     "end_time": "2025-02-03T20:22:28.477901",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.469058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocess by DeepLabV3 Remove Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10dacb20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:28.497839Z",
     "iopub.status.busy": "2025-02-03T20:22:28.497458Z",
     "iopub.status.idle": "2025-02-03T20:22:28.502916Z",
     "shell.execute_reply": "2025-02-03T20:22:28.501719Z"
    },
    "papermill": {
     "duration": 0.017204,
     "end_time": "2025-02-03T20:22:28.504706",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.487502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torchvision.transforms as T\n",
    "# from PIL import Image\n",
    "# from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n",
    "# import numpy as np\n",
    "\n",
    "# # Set data paths\n",
    "# train_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/train_images\"\n",
    "# val_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/val_images\"\n",
    "# test_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/test_images/mistery_cat\"\n",
    "# output_dir = \"/kaggle/working/processed_images\"\n",
    "\n",
    "# # Create output directory\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Load the pretrained DeepLabV3+ model (ResNet101 backbone)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Load pretrained model with correct weights handling\n",
    "# weights = DeepLabV3_ResNet101_Weights.DEFAULT\n",
    "# model = deeplabv3_resnet101(weights=weights).to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # Image preprocessing (fixed \"mean\" and \"std\" issue)\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"Preprocess the input image for the DeepLabV3+ model.\"\"\"\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     transform = T.Compose([\n",
    "#         T.Resize((512, 512)),\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize(mean=weights.transforms().mean, std=weights.transforms().std),  # FIXED\n",
    "#     ])\n",
    "#     return transform(image).unsqueeze(0), image\n",
    "\n",
    "# # Post-processing: apply the segmentation mask to the original image\n",
    "# def apply_mask(image, mask):\n",
    "#     \"\"\"Apply the segmentation mask to the original image to remove the background.\"\"\"\n",
    "#     mask = mask.cpu().numpy()\n",
    "#     mask = (mask > 0.5).astype(np.uint8)  # Binarize the mask\n",
    "#     mask = Image.fromarray(mask * 255).resize(image.size, Image.BILINEAR)\n",
    "#     mask = np.array(mask) / 255\n",
    "#     image_np = np.array(image)\n",
    "#     result = (image_np * np.expand_dims(mask, axis=-1)).astype(np.uint8)\n",
    "#     return Image.fromarray(result)\n",
    "\n",
    "# def process_directory(input_dir, output_subdir):\n",
    "#     \"\"\"Process all images in a directory, including subdirectories, and save the results.\"\"\"\n",
    "#     output_path = os.path.join(output_dir, output_subdir)\n",
    "#     os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "#     print(f\"üîç Removing images Background in: {input_dir}\")\n",
    "\n",
    "#     # Walk through all subdirectories and files\n",
    "#     for root, _, files in os.walk(input_dir):\n",
    "#         for filename in files:\n",
    "#             file_path = os.path.join(root, filename)\n",
    "\n",
    "#             # Process only image files\n",
    "#             if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "#                 # Recreate subdirectory structure in output\n",
    "#                 relative_path = os.path.relpath(root, input_dir)\n",
    "#                 sub_output_path = os.path.join(output_path, relative_path)\n",
    "#                 os.makedirs(sub_output_path, exist_ok=True)\n",
    "\n",
    "#                 try:\n",
    "#                     # Preprocess the image\n",
    "#                     input_tensor, original_image = preprocess_image(file_path)\n",
    "#                     input_tensor = input_tensor.to(device)\n",
    "\n",
    "#                     # Inference\n",
    "#                     with torch.no_grad():\n",
    "#                         output = model(input_tensor)['out']\n",
    "#                     mask = torch.argmax(output.squeeze(), dim=0)\n",
    "\n",
    "#                     # Apply the mask\n",
    "#                     result_image = apply_mask(original_image, mask)\n",
    "\n",
    "#                     # Save the result\n",
    "#                     result_path = os.path.join(sub_output_path, filename)\n",
    "#                     result_image.save(result_path)\n",
    "#                     # print(f\"‚úÖ Processed and saved: {result_path}\")\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"‚ùå Error processing {file_path}: {e}\")  # Debugging output\n",
    "\n",
    "# # Process training, validation, and test datasets\n",
    "# print(\"Processing training images...\")\n",
    "# process_directory(train_dir, \"train_images\")\n",
    "\n",
    "# print(\"Processing validation images...\")\n",
    "# process_directory(val_dir, \"val_images\")\n",
    "\n",
    "# print(\"Processing test images...\")\n",
    "# process_directory(test_dir, \"test_images\")\n",
    "\n",
    "# print(f\"All images processed and saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c394160e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:28.523681Z",
     "iopub.status.busy": "2025-02-03T20:22:28.523285Z",
     "iopub.status.idle": "2025-02-03T20:22:28.528535Z",
     "shell.execute_reply": "2025-02-03T20:22:28.527466Z"
    },
    "papermill": {
     "duration": 0.016759,
     "end_time": "2025-02-03T20:22:28.530485",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.513726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torchvision.transforms as T\n",
    "# from PIL import Image\n",
    "# from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n",
    "# import numpy as np\n",
    "\n",
    "# # Paths\n",
    "# train_dir = \"/kaggle/working/New_Split_Dataset/train_images\"\n",
    "# augmented_train_dir = \"/kaggle/working/Augmented_Processed_Dataset/train_images\"\n",
    "# output_train_dir = \"/kaggle/working/Augmented_Processed_RemovedBackground_Dataset/train_images\"\n",
    "# val_dir = \"/kaggle/working/New_Split_Dataset/val_images\"\n",
    "# output_val_dir = \"/kaggle/working/Augmented_Processed_RemovedBackground_Dataset/val_images\"\n",
    "# test_dir = \"/kaggle/input/bdma-07-competition/BDMA7_project_files/test_images\"\n",
    "# output_test_dir = \"/kaggle/working/Augmented_Processed_RemovedBackground_Dataset/test_images\"\n",
    "\n",
    "# # Create output directories\n",
    "# os.makedirs(output_train_dir, exist_ok=True)\n",
    "# os.makedirs(output_val_dir, exist_ok=True)\n",
    "# os.makedirs(output_test_dir, exist_ok=True)\n",
    "\n",
    "# # Load pretrained DeepLabV3+ model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# weights = DeepLabV3_ResNet101_Weights.DEFAULT\n",
    "# model = deeplabv3_resnet101(weights=weights).to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # Image preprocessing\n",
    "# def preprocess_image(image_path):\n",
    "#     \"\"\"Preprocess the input image for the DeepLabV3+ model.\"\"\"\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     transform = T.Compose([\n",
    "#         T.Resize((512, 512)),\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize(mean=weights.transforms().mean, std=weights.transforms().std),\n",
    "#     ])\n",
    "#     return transform(image).unsqueeze(0), image\n",
    "\n",
    "# # Post-processing: apply the segmentation mask to the original image\n",
    "# def apply_mask(image, mask):\n",
    "#     \"\"\"Apply the segmentation mask to the original image to remove the background.\"\"\"\n",
    "#     mask = mask.cpu().numpy()\n",
    "#     mask = (mask > 0.5).astype(np.uint8)  # Binarize the mask\n",
    "#     mask = Image.fromarray(mask * 255).resize(image.size, Image.BILINEAR)\n",
    "#     mask = np.array(mask) / 255\n",
    "#     image_np = np.array(image)\n",
    "#     result = (image_np * np.expand_dims(mask, axis=-1)).astype(np.uint8)\n",
    "#     return Image.fromarray(result)\n",
    "\n",
    "# # Process images and remove background\n",
    "# def process_directory(input_dirs, output_dir):\n",
    "#     \"\"\"Process all images in multiple directories and save results.\"\"\"\n",
    "#     for input_dir in input_dirs:\n",
    "#         print(f\"Removing background from images in: {input_dir}\")\n",
    "#         for root, _, files in os.walk(input_dir):\n",
    "#             for filename in files:\n",
    "#                 file_path = os.path.join(root, filename)\n",
    "\n",
    "#                 # Process only image files\n",
    "#                 if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "#                     # Recreate subdirectory structure in output\n",
    "#                     relative_path = os.path.relpath(root, input_dir)\n",
    "#                     sub_output_path = os.path.join(output_dir, relative_path)\n",
    "#                     os.makedirs(sub_output_path, exist_ok=True)\n",
    "\n",
    "#                     try:\n",
    "#                         # Preprocess the image\n",
    "#                         input_tensor, original_image = preprocess_image(file_path)\n",
    "#                         input_tensor = input_tensor.to(device)\n",
    "\n",
    "#                         # Inference\n",
    "#                         with torch.no_grad():\n",
    "#                             output = model(input_tensor)['out']\n",
    "#                         mask = torch.argmax(output.squeeze(), dim=0)\n",
    "\n",
    "#                         # Apply the mask\n",
    "#                         result_image = apply_mask(original_image, mask)\n",
    "\n",
    "#                         # Save the result\n",
    "#                         result_path = os.path.join(sub_output_path, filename)\n",
    "#                         result_image.save(result_path)\n",
    "\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# # Process training set\n",
    "# process_directory([train_dir, augmented_train_dir], output_train_dir)\n",
    "\n",
    "# # Process validation set\n",
    "# process_directory([val_dir], output_val_dir)\n",
    "\n",
    "# # Process test set\n",
    "# process_directory([test_dir], output_test_dir)\n",
    "\n",
    "# print(f\"All images processed and saved in {output_train_dir}, {output_val_dir}, and {output_test_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae3b2f",
   "metadata": {
    "papermill": {
     "duration": 0.008345,
     "end_time": "2025-02-03T20:22:28.547847",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.539502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Vision Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "675b11a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:28.566680Z",
     "iopub.status.busy": "2025-02-03T20:22:28.566316Z",
     "iopub.status.idle": "2025-02-03T20:22:28.570216Z",
     "shell.execute_reply": "2025-02-03T20:22:28.569083Z"
    },
    "papermill": {
     "duration": 0.015428,
     "end_time": "2025-02-03T20:22:28.571820",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.556392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dir = \"/kaggle/working/preprocessed02_images\"\n",
    "# val_dir = \"/kaggle/working/processed_images/val_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f04ccad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:28.591000Z",
     "iopub.status.busy": "2025-02-03T20:22:28.590656Z",
     "iopub.status.idle": "2025-02-03T20:22:28.594644Z",
     "shell.execute_reply": "2025-02-03T20:22:28.593531Z"
    },
    "papermill": {
     "duration": 0.015472,
     "end_time": "2025-02-03T20:22:28.596352",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.580880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # model_name = \"google/vit-base-patch16-224-in21k\"  # 224x224 InputÔºå16x16 Chunk\n",
    "# # Change to ViT-Large\n",
    "# model_name = \"google/vit-large-patch16-224-in21k\"\n",
    "# processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "# model = ViTForImageClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=20,  # Number of Classes\n",
    "#     ignore_mismatched_sizes=True  \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03fff7",
   "metadata": {
    "papermill": {
     "duration": 0.008157,
     "end_time": "2025-02-03T20:22:28.614049",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.605892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Freeze feature layers of ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9810945",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:28.632456Z",
     "iopub.status.busy": "2025-02-03T20:22:28.632080Z",
     "iopub.status.idle": "2025-02-03T20:22:28.636106Z",
     "shell.execute_reply": "2025-02-03T20:22:28.634980Z"
    },
    "papermill": {
     "duration": 0.01536,
     "end_time": "2025-02-03T20:22:28.637882",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.622522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Freeze all feature layers of ViT (train only the classification head)\n",
    "# for param in model.vit.parameters():\n",
    "#     # param.requires_grad = False\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Thaw the classification headers separately\n",
    "# for param in model.classifier.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae93d33",
   "metadata": {
    "papermill": {
     "duration": 0.008301,
     "end_time": "2025-02-03T20:22:28.654951",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.646650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Set layer-wise learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a6770e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:28.674334Z",
     "iopub.status.busy": "2025-02-03T20:22:28.673884Z",
     "iopub.status.idle": "2025-02-03T20:22:28.678021Z",
     "shell.execute_reply": "2025-02-03T20:22:28.676933Z"
    },
    "papermill": {
     "duration": 0.01612,
     "end_time": "2025-02-03T20:22:28.679875",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.663755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Set layer-wise learning rate (feature layer learning rate is lower)\n",
    "# optimizer = AdamW(\n",
    "#     [\n",
    "#         {\"params\": model.vit.parameters(), \"lr\": 1e-5},  # Feature extraction layer\n",
    "#         {\"params\": model.classifier.parameters(), \"lr\": 3e-4}  # Classification Header\n",
    "#     ],\n",
    "#     weight_decay=0.01\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e464190a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:28.699145Z",
     "iopub.status.busy": "2025-02-03T20:22:28.698758Z",
     "iopub.status.idle": "2025-02-03T20:22:28.702865Z",
     "shell.execute_reply": "2025-02-03T20:22:28.701713Z"
    },
    "papermill": {
     "duration": 0.015702,
     "end_time": "2025-02-03T20:22:28.704677",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.688975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Verify that the classification layer is randomly initialized (correct state should be True)\n",
    "# print(model.classifier.weight.mean().item())  # Should be close to 0 (normal distribution initialization)\n",
    "# print(model.classifier.bias.mean().item())    # Should be close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbbc412",
   "metadata": {
    "papermill": {
     "duration": 0.008638,
     "end_time": "2025-02-03T20:22:28.722260",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.713622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Check Submission Classes Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf6c1ea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:28.741848Z",
     "iopub.status.busy": "2025-02-03T20:22:28.741403Z",
     "iopub.status.idle": "2025-02-03T20:22:28.745806Z",
     "shell.execute_reply": "2025-02-03T20:22:28.744531Z"
    },
    "papermill": {
     "duration": 0.016626,
     "end_time": "2025-02-03T20:22:28.747916",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.731290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission_class_order = [\n",
    "#     'Groove_billed_Ani',\n",
    "#     'Red_winged_Blackbird',\n",
    "#     'Rusty_Blackbird',\n",
    "#     'Gray_Catbird',\n",
    "#     'Brandt_Cormorant',\n",
    "#     'Eastern_Towhee',\n",
    "#     'Indigo_Bunting',\n",
    "#     'Brewer_Blackbird',\n",
    "#     'Painted_Bunting',\n",
    "#     'Bobolink',\n",
    "#     'Lazuli_Bunting',\n",
    "#     'Yellow_headed_Blackbird',\n",
    "#     'American_Crow',\n",
    "#     'Fish_Crow',\n",
    "#     'Brown_Creeper',\n",
    "#     'Yellow_billed_Cuckoo',\n",
    "#     'Yellow_breasted_Chat',\n",
    "#     'Black_billed_Cuckoo',\n",
    "#     'Gray_crowned_Rosy_Finch',\n",
    "#     'Bronzed_Cowbird'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d32bee96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:28.767435Z",
     "iopub.status.busy": "2025-02-03T20:22:28.767026Z",
     "iopub.status.idle": "2025-02-03T20:22:28.772029Z",
     "shell.execute_reply": "2025-02-03T20:22:28.770980Z"
    },
    "papermill": {
     "duration": 0.016934,
     "end_time": "2025-02-03T20:22:28.773837",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.756903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class BirdDataset(Dataset):\n",
    "#     def __init__(self, main_dir, transform=None):\n",
    "#         self.dataset = ImageFolder(root=main_dir, transform=transform)\n",
    "#         self.class_to_idx = self.dataset.class_to_idx\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         image, label = self.dataset[idx]\n",
    "#         return image, label\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.RandomCrop(224),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "# ])\n",
    "\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "# ])\n",
    "\n",
    "# train_dataset = BirdDataset(train_dir, transform=train_transform)\n",
    "# val_dataset = BirdDataset(val_dir, transform=val_transform)\n",
    "\n",
    "# def validate_class_order(train_class_order, submission_order):\n",
    "#     \"\"\"Make sure the category names and order of both lists are exactly the same\"\"\"\n",
    "#     if len(train_class_order) != len(submission_order):\n",
    "#         raise ValueError(f\"The number of categories does not match! Training set: {len(train_class_order)}, Submission Requirements: {len(submission_order)}\")\n",
    "    \n",
    "#     for train_cls, sub_cls in zip(train_class_order, submission_order):\n",
    "#         if train_cls != sub_cls:\n",
    "#             raise ValueError(f\"Inconsistent order: training set '{train_cls}' vs Submission Requirements '{sub_cls}'\")\n",
    "#     return True\n",
    "\n",
    "# train_class_order = sorted(train_dataset.class_to_idx.keys())\n",
    "\n",
    "# try:\n",
    "#     validate_class_order(train_class_order, submission_class_order)\n",
    "# except ValueError as e:\n",
    "#     print(\"Category order inconsistency detected, automatically correcting...\")\n",
    "#     from torchvision.datasets import DatasetFolder\n",
    "    \n",
    "#     class OrderedImageFolder(DatasetFolder):\n",
    "#         \"\"\"Forces the data sets of categories to be loaded in a specified order\"\"\"\n",
    "#         def __init__(self, root, class_order, transform=None):\n",
    "#             self.class_order = class_order\n",
    "#             super().__init__(\n",
    "#                 root,\n",
    "#                 loader=lambda x: Image.open(x).convert(\"RGB\"),\n",
    "#                 extensions=('jpg', 'jpeg', 'png'),\n",
    "#                 transform=transform,\n",
    "#                 target_transform=None\n",
    "#             )\n",
    "            \n",
    "#         def find_classes(self, directory):\n",
    "#             classes = self.class_order \n",
    "#             class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "#             return classes, class_to_idx\n",
    "    \n",
    "#     train_dataset = OrderedImageFolder(\n",
    "#         train_dir, \n",
    "#         class_order=submission_class_order,\n",
    "#         transform=train_transform\n",
    "#     )\n",
    "#     val_dataset = OrderedImageFolder(\n",
    "#         val_dir,\n",
    "#         class_order=submission_class_order,\n",
    "#         transform=val_transform\n",
    "#     )\n",
    "    \n",
    "#     print(\"Corrected category orderÔºö\", train_dataset.classes)\n",
    "    \n",
    "# # Category index validation (ensuring consistency with submission format)\n",
    "# assert sorted(train_dataset.class_to_idx.keys()) == sorted(submission_class_order), \"Category order mismatchÔºÅ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baebce8",
   "metadata": {
    "papermill": {
     "duration": 0.008331,
     "end_time": "2025-02-03T20:22:28.791396",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.783065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d21923c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:28.812102Z",
     "iopub.status.busy": "2025-02-03T20:22:28.811733Z",
     "iopub.status.idle": "2025-02-03T20:22:28.816825Z",
     "shell.execute_reply": "2025-02-03T20:22:28.815720Z"
    },
    "papermill": {
     "duration": 0.018006,
     "end_time": "2025-02-03T20:22:28.818663",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.800657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Define Dataloaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# # Define loss function and move model to device\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Function to train a single epoch\n",
    "# def train_epoch(model, loader, optimizer, scaler):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "    \n",
    "#     for images, labels in loader:\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         with torch.amp.autocast(device_type='cuda'):\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs.logits, labels)\n",
    "        \n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "        \n",
    "#         total_loss += loss.item() * images.size(0)\n",
    "#         preds = torch.argmax(outputs.logits, dim=1)\n",
    "#         correct += (preds == labels).sum().item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(loader.dataset)\n",
    "#     accuracy = correct / len(loader.dataset)\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "# # Function to validate the model\n",
    "# def validate(model, loader):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "            \n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "#             total_loss += loss.item() * images.size(0)\n",
    "#             preds = torch.argmax(outputs.logits, dim=1)\n",
    "#             correct += (preds == labels).sum().item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(loader.dataset)\n",
    "#     accuracy = correct / len(loader.dataset)\n",
    "#     return avg_loss, accuracy\n",
    "\n",
    "# scaler = torch.amp.GradScaler()\n",
    "# history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "# best_val_loss = float('inf')\n",
    "# best_model_state = None\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     train_loss, train_acc = train_epoch(model, train_loader, optimizer, scaler)\n",
    "#     val_loss, val_acc = validate(model, val_loader)\n",
    "    \n",
    "#     history['train_loss'].append(train_loss)\n",
    "#     history['val_loss'].append(val_loss)\n",
    "#     history['train_acc'].append(train_acc)\n",
    "#     history['val_acc'].append(val_acc)\n",
    "    \n",
    "#     print(f\"Epoch {epoch+1:02d}:\")\n",
    "#     print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
    "#     print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\\n\")\n",
    "    \n",
    "#     # Save the best model\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         best_model_state = model.state_dict()\n",
    "\n",
    "# # Load the best model at the end of training\n",
    "# best_model_path = \"/kaggle/working/best_vit_model.pth\"\n",
    "# if best_model_state:\n",
    "#     model.load_state_dict(best_model_state)\n",
    "#     torch.save(best_model_state, best_model_path)\n",
    "#     print(\"Best model loaded from training with lowest validation loss.\")\n",
    "\n",
    "# # Plot training history\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history['train_loss'], label='Train')\n",
    "# plt.plot(history['val_loss'], label='Validation')\n",
    "# plt.title('Loss Curve')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history['train_acc'], label='Train')\n",
    "# plt.plot(history['val_acc'], label='Validation')\n",
    "# plt.title('Accuracy Curve')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce582dc",
   "metadata": {
    "papermill": {
     "duration": 0.008158,
     "end_time": "2025-02-03T20:22:28.835878",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.827720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff5ef350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:28.854529Z",
     "iopub.status.busy": "2025-02-03T20:22:28.854109Z",
     "iopub.status.idle": "2025-02-03T20:22:28.858468Z",
     "shell.execute_reply": "2025-02-03T20:22:28.857509Z"
    },
    "papermill": {
     "duration": 0.01574,
     "end_time": "2025-02-03T20:22:28.860226",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.844486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save_dir = \"/kaggle/working/vit_model\"\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# torch.save(model.state_dict(), os.path.join(save_dir, \"vit_model_weights.pth\"))\n",
    "\n",
    "# processor.save_pretrained(save_dir)\n",
    "\n",
    "# class_info = {\n",
    "#     \"class_order\": submission_class_order,\n",
    "#     \"class_to_idx\": train_dataset.class_to_idx,\n",
    "#     \"idx_to_class\": {v: k for k, v in train_dataset.class_to_idx.items()}\n",
    "# }\n",
    "\n",
    "# torch.save(class_info, os.path.join(save_dir, \"class_info.pth\"))\n",
    "\n",
    "# train_config = {\n",
    "#     \"epochs_trained\": len(history['train_loss']),\n",
    "#     \"best_val_acc\": max(history['val_acc']),\n",
    "#     \"optimizer_state\": optimizer.state_dict()\n",
    "# }\n",
    "\n",
    "# torch.save(train_config, os.path.join(save_dir, \"train_config.pth\"))\n",
    "\n",
    "# print(f\"Model Saved toÔºö{save_dir}\")\n",
    "\n",
    "# # Second Hugging Face Format Save\n",
    "# model.save_pretrained(save_dir)\n",
    "# processor.save_pretrained(save_dir)\n",
    "\n",
    "# with open(os.path.join(save_dir, \"class_info.txt\"), \"w\") as f:\n",
    "#     f.write(\"\\n\".join(submission_class_order))\n",
    "\n",
    "# print(\"HuggingFace Format Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7969219",
   "metadata": {
    "papermill": {
     "duration": 0.008491,
     "end_time": "2025-02-03T20:22:28.877889",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.869398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test and Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42821ec1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:28.897400Z",
     "iopub.status.busy": "2025-02-03T20:22:28.896959Z",
     "iopub.status.idle": "2025-02-03T20:22:28.901918Z",
     "shell.execute_reply": "2025-02-03T20:22:28.900798Z"
    },
    "papermill": {
     "duration": 0.016892,
     "end_time": "2025-02-03T20:22:28.903756",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.886864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Set device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Define class order for submission\n",
    "# submission_class_order = [\n",
    "#     'Groove_billed_Ani', 'Red_winged_Blackbird', 'Rusty_Blackbird', 'Gray_Catbird',\n",
    "#     'Brandt_Cormorant', 'Eastern_Towhee', 'Indigo_Bunting', 'Brewer_Blackbird',\n",
    "#     'Painted_Bunting', 'Bobolink', 'Lazuli_Bunting', 'Yellow_headed_Blackbird',\n",
    "#     'American_Crow', 'Fish_Crow', 'Brown_Creeper', 'Yellow_billed_Cuckoo',\n",
    "#     'Yellow_breasted_Chat', 'Black_billed_Cuckoo', 'Gray_crowned_Rosy_Finch', 'Bronzed_Cowbird'\n",
    "# ]\n",
    "\n",
    "# # Image transformation for validation\n",
    "# processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "# ])\n",
    "\n",
    "# # Custom Dataset class for test images\n",
    "# class CompetitionTestDataset(Dataset):\n",
    "#     def __init__(self, test_dir, transform=None):\n",
    "#         self.test_dir = test_dir\n",
    "#         self.image_files = sorted(os.listdir(test_dir))\n",
    "#         self.image_paths = [os.path.join(test_dir, f) for f in self.image_files]\n",
    "#         self.transform = transform\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image, os.path.basename(self.image_paths[idx])\n",
    "\n",
    "# # Load best saved model\n",
    "# def load_best_model(model, model_path):\n",
    "#     model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "#     model.eval()\n",
    "#     return model\n",
    "\n",
    "# # Generate predictions using the best model\n",
    "# def generate_submission(test_dir, best_model_path, output_csv=\"submission.csv\"):\n",
    "#     test_dataset = CompetitionTestDataset(test_dir, transform=val_transform)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "    \n",
    "#     config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=len(submission_class_order))\n",
    "#     model = ViTForImageClassification(config).to(device)\n",
    "#     model = load_best_model(model, best_model_path)\n",
    "    \n",
    "#     filenames = []\n",
    "#     predictions = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for images, paths in test_loader:\n",
    "#             outputs = model(images.to(device))\n",
    "#             batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            \n",
    "#             filenames.extend(paths)\n",
    "#             predictions.extend(batch_preds.tolist())\n",
    "    \n",
    "#     submission_df = pd.DataFrame({\n",
    "#         'path': filenames,\n",
    "#         'class_idx': predictions\n",
    "#     })\n",
    "    \n",
    "#     print(\"\\nValidation Results:\")\n",
    "#     print(f\"Total Samples: {len(submission_df)}\")\n",
    "#     print(f\"Number of unique file names: {submission_df['path'].nunique()}\")\n",
    "#     print(f\"Predicted category distribution:\\n{submission_df['class_idx'].value_counts().sort_index()}\")\n",
    "    \n",
    "#     submission_df.to_csv(output_csv, index=False)\n",
    "#     print(f\"\\nSubmission saved to: {output_csv}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_dir = \"/kaggle/working/processed_images/test_images\"\n",
    "#     best_model_path = \"/kaggle/working/best_vit_model.pth\"  # Use best saved model\n",
    "    \n",
    "#     generate_submission(test_dir, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "045f97fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T20:22:28.922468Z",
     "iopub.status.busy": "2025-02-03T20:22:28.922066Z",
     "iopub.status.idle": "2025-02-03T20:22:28.925910Z",
     "shell.execute_reply": "2025-02-03T20:22:28.924857Z"
    },
    "papermill": {
     "duration": 0.014757,
     "end_time": "2025-02-03T20:22:28.927429",
     "exception": false,
     "start_time": "2025-02-03T20:22:28.912672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# shutil.rmtree('/kaggle/working/preprocessed02_images', ignore_errors=True)\n",
    "# shutil.rmtree('/kaggle/working/preprocessed_images', ignore_errors=True) \n",
    "# shutil.rmtree('/kaggle/working/processed_images', ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10794348,
     "sourceId": 91120,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 74.621174,
   "end_time": "2025-02-03T20:22:31.888358",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-03T20:21:17.267184",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
