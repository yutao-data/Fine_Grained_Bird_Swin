Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-large-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/gpfs/users/yutaoc/ml_task/main.py", line 29, in <module>
    main()
  File "/gpfs/users/yutaoc/ml_task/main.py", line 22, in main
    model = train_and_evaluate()
            ^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/users/yutaoc/ml_task/src/training/train.py", line 46, in train_and_evaluate
    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scaler, device)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/users/yutaoc/ml_task/src/utils/utils.py", line 24, in train_epoch
    scaler.step(optimizer)
  File "/gpfs/users/yutaoc/miniconda3/envs/bird/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 454, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/users/yutaoc/miniconda3/envs/bird/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/users/yutaoc/miniconda3/envs/bird/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
               ^^^^^^^^
RuntimeError: CUDA error: unknown error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

slurmstepd: error: *** JOB 9082152 ON ruche-gpu16 CANCELLED AT 2025-01-30T22:16:36 ***
slurmstepd: error: proctrack_p_wait: Unable to destroy container 25430 in cgroup plugin, giving up after 128 sec
