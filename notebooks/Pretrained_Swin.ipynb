{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91120,"databundleVersionId":10794348,"sourceType":"competition"},{"sourceId":10660482,"sourceType":"datasetVersion","datasetId":6601774}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"! pip install transformers timm albumentations --quiet\n! pip install ipywidgets --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:45:10.655859Z","iopub.execute_input":"2025-02-06T13:45:10.656213Z","iopub.status.idle":"2025-02-06T13:45:17.414623Z","shell.execute_reply.started":"2025-02-06T13:45:10.656189Z","shell.execute_reply":"2025-02-06T13:45:17.413667Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# Standard Libraries\nimport os\nimport random\nfrom collections import Counter\n\n# Numerical and Data Handling Libraries\nimport numpy as np\nimport pandas as pd\n\n# Image Processing Libraries\nimport cv2\nfrom PIL import Image\n\n# Plotting and Visualization\nimport matplotlib.pyplot as plt\n\n# PyTorch and Torchvision\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\n\n# Transformers for Vision Models\nfrom transformers import ViTConfig, ViTForImageClassification, ViTImageProcessor\n\n# Utility Libraries\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score\n\n# jupyter nbextension enable --py widgetsnbextension\nfrom google.colab import output\noutput.enable_custom_widget_manager()\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:45:17.416181Z","iopub.execute_input":"2025-02-06T13:45:17.416515Z","iopub.status.idle":"2025-02-06T13:45:17.422987Z","shell.execute_reply.started":"2025-02-06T13:45:17.416481Z","shell.execute_reply":"2025-02-06T13:45:17.422172Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"# Data Import","metadata":{}},{"cell_type":"code","source":"train_dir = \"/kaggle/input/croped-processed-augmented-bird-dataset/archive/Augmented_CropImage_Processed_Dataset/train_images\"\nval_dir = \"/kaggle/input/croped-processed-augmented-bird-dataset/archive/Augmented_CropImage_Processed_Dataset/val_images\"\ntest_dir = \"/kaggle/input/croped-processed-augmented-bird-dataset/archive/Augmented_CropImage_Processed_Dataset/test_images/mistery_cat\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:45:17.424628Z","iopub.execute_input":"2025-02-06T13:45:17.424883Z","iopub.status.idle":"2025-02-06T13:45:17.442955Z","shell.execute_reply.started":"2025-02-06T13:45:17.424863Z","shell.execute_reply":"2025-02-06T13:45:17.442150Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"# Vision Transformer Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoImageProcessor, SwinForImageClassification\n\ndef load_model_and_processor(model_name: str, num_labels: int = 20):\n    processor = AutoImageProcessor.from_pretrained(model_name)\n    model = SwinForImageClassification.from_pretrained(\n        model_name,\n        num_labels=num_labels,\n        ignore_mismatched_sizes=True  \n    )\n\n    model.classifier = torch.nn.Linear(model.config.hidden_size, num_labels)\n    torch.nn.init.xavier_uniform_(model.classifier.weight)  \n    torch.nn.init.zeros_(model.classifier.bias)\n\n    return processor, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:45:17.444442Z","iopub.execute_input":"2025-02-06T13:45:17.444699Z","iopub.status.idle":"2025-02-06T13:45:17.458398Z","shell.execute_reply.started":"2025-02-06T13:45:17.444679Z","shell.execute_reply":"2025-02-06T13:45:17.457547Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# import timm\n# model = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:45:17.459258Z","iopub.execute_input":"2025-02-06T13:45:17.459574Z","iopub.status.idle":"2025-02-06T13:45:17.472394Z","shell.execute_reply.started":"2025-02-06T13:45:17.459546Z","shell.execute_reply":"2025-02-06T13:45:17.471724Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"## Freeze all feature layers of Swin","metadata":{}},{"cell_type":"code","source":"def configure_trainable_layers(model):\n    \"\"\"\n    Configures the Swin model by freezing all layers except for the last stage and the classifier.\n    :param model: SwinForImageClassification model.\n    \"\"\"\n    for param in model.swin.parameters():\n        param.requires_grad = False\n    \n    if hasattr(model.swin, \"stages\"):\n        for param in model.swin.stages[-1].parameters():\n            param.requires_grad = True\n        print(\"Unfroze the last stage of the Swin backbone.\")\n    \n    for param in model.classifier.parameters():\n        param.requires_grad = True\n\n# optimizer_setup.py\nfrom torch.optim import AdamW  # Use PyTorch's AdamW","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:45:17.473149Z","iopub.execute_input":"2025-02-06T13:45:17.473421Z","iopub.status.idle":"2025-02-06T13:45:17.486602Z","shell.execute_reply.started":"2025-02-06T13:45:17.473394Z","shell.execute_reply":"2025-02-06T13:45:17.485818Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## Set layer-wise learning rate","metadata":{}},{"cell_type":"code","source":"import torch.optim.lr_scheduler as lr_scheduler\n\ndef create_optimizer(model, feature_lr: float = 1e-5, classifier_lr: float = 3e-4):\n    \"\"\"\n    Creates an AdamW optimizer using PyTorch's implementation.\n    :param model: SwinForImageClassification model.\n    :param feature_lr: Learning rate for feature extractor layers.\n    :param classifier_lr: Learning rate for classifier head.\n    :return: Tuple (optimizer, scheduler)\n    \"\"\"\n    optimizer = AdamW(\n        [\n            {\"params\": model.swin.parameters(), \"lr\": feature_lr, \"weight_decay\": 0.01},\n            {\"params\": model.classifier.parameters(), \"lr\": classifier_lr, \"weight_decay\": 0.01}\n        ]\n    )\n    \n    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n    \n    return optimizer, scheduler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:45:17.487439Z","iopub.execute_input":"2025-02-06T13:45:17.487659Z","iopub.status.idle":"2025-02-06T13:45:17.505008Z","shell.execute_reply.started":"2025-02-06T13:45:17.487630Z","shell.execute_reply":"2025-02-06T13:45:17.504221Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"MODEL_NAME = \"Emiel/cub-200-bird-classifier-swin\"\n# MODEL_NAME = \"swin_base_patch4_window12_384\"\nNUM_LABELS = 20\n\nprocessor, model = load_model_and_processor(MODEL_NAME, NUM_LABELS)\nconfigure_trainable_layers(model)\noptimizer, scheduler = create_optimizer(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:45:17.506981Z","iopub.execute_input":"2025-02-06T13:45:17.507227Z","iopub.status.idle":"2025-02-06T13:45:19.409343Z","shell.execute_reply.started":"2025-02-06T13:45:17.507207Z","shell.execute_reply":"2025-02-06T13:45:19.408645Z"}},"outputs":[{"name":"stderr","text":"Some weights of SwinForImageClassification were not initialized from the model checkpoint at Emiel/cub-200-bird-classifier-swin and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([200]) in the checkpoint and torch.Size([20]) in the model instantiated\n- classifier.weight: found shape torch.Size([200, 1536]) in the checkpoint and torch.Size([20, 1536]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"submission_class_order = [\n    'Groove_billed_Ani',\n    'Red_winged_Blackbird',\n    'Rusty_Blackbird',\n    'Gray_Catbird',\n    'Brandt_Cormorant',\n    'Eastern_Towhee',\n    'Indigo_Bunting',\n    'Brewer_Blackbird',\n    'Painted_Bunting',\n    'Bobolink',\n    'Lazuli_Bunting',\n    'Yellow_headed_Blackbird',\n    'American_Crow',\n    'Fish_Crow',\n    'Brown_Creeper',\n    'Yellow_billed_Cuckoo',\n    'Yellow_breasted_Chat',\n    'Black_billed_Cuckoo',\n    'Gray_crowned_Rosy_Finch',\n    'Bronzed_Cowbird'\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:45:19.410277Z","iopub.execute_input":"2025-02-06T13:45:19.410572Z","iopub.status.idle":"2025-02-06T13:45:19.414251Z","shell.execute_reply.started":"2025-02-06T13:45:19.410541Z","shell.execute_reply":"2025-02-06T13:45:19.413463Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass BirdDataset(Dataset):\n    def __init__(self, main_dir, transform=None):\n        self.dataset = ImageFolder(root=main_dir, transform=transform)\n        self.class_to_idx = self.dataset.class_to_idx\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        image, label = self.dataset[idx]\n        return image, label\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((420, 420)),        \n    transforms.RandomResizedCrop(384, scale=(0.8, 1.0)),  \n    transforms.RandomHorizontalFlip(p=0.5),  \n    transforms.RandomVerticalFlip(p=0.2),    \n    transforms.RandomRotation(degrees=15),   \n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  \n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),  \n    transforms.RandomGrayscale(p=0.1),  \n    transforms.ToTensor(),  \n    transforms.RandomErasing(p=0.2, scale=(0.02, 0.2), ratio=(0.3, 3.3)),  \n    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n])\n\n\nval_transform = transforms.Compose([\n    transforms.Resize((384, 384)),  \n    transforms.ToTensor(),\n    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n])\n\ntrain_dataset = BirdDataset(train_dir, transform=train_transform)\nval_dataset = BirdDataset(val_dir, transform=val_transform)\n\ndef validate_class_order(train_class_order, submission_order):\n    \"\"\"Make sure the category names and order of both lists are exactly the same\"\"\"\n    if len(train_class_order) != len(submission_order):\n        raise ValueError(f\"The number of categories does not match! Training set: {len(train_class_order)}, Submission Requirements: {len(submission_order)}\")\n    \n    for train_cls, sub_cls in zip(train_class_order, submission_order):\n        if train_cls != sub_cls:\n            raise ValueError(f\"Inconsistent order: training set '{train_cls}' vs Submission Requirements '{sub_cls}'\")\n    return True\n\ntrain_class_order = sorted(train_dataset.class_to_idx.keys())\n\ntry:\n    validate_class_order(train_class_order, submission_class_order)\nexcept ValueError as e:\n    print(\"Category order inconsistency detected, automatically correcting...\")\n    from torchvision.datasets import DatasetFolder\n    \n    class OrderedImageFolder(DatasetFolder):\n        \"\"\"Forces the data sets of categories to be loaded in a specified order\"\"\"\n        def __init__(self, root, class_order, transform=None):\n            self.class_order = class_order\n            super().__init__(\n                root,\n                loader=lambda x: Image.open(x).convert(\"RGB\"),\n                extensions=('jpg', 'jpeg', 'png'),\n                transform=transform,\n                target_transform=None\n            )\n            \n        def find_classes(self, directory):\n            classes = self.class_order \n            class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n            return classes, class_to_idx\n    \n    train_dataset = OrderedImageFolder(\n        train_dir, \n        class_order=submission_class_order,\n        transform=train_transform\n    )\n    val_dataset = OrderedImageFolder(\n        val_dir,\n        class_order=submission_class_order,\n        transform=val_transform\n    )\n    \n    print(\"Corrected category order：\", train_dataset.classes)\n    \n# Category index validation (ensuring consistency with submission format)\nassert sorted(train_dataset.class_to_idx.keys()) == sorted(submission_class_order), \"Category order mismatch！\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:45:19.415205Z","iopub.execute_input":"2025-02-06T13:45:19.415469Z","iopub.status.idle":"2025-02-06T13:45:19.530330Z","shell.execute_reply.started":"2025-02-06T13:45:19.415449Z","shell.execute_reply":"2025-02-06T13:45:19.529541Z"}},"outputs":[{"name":"stdout","text":"Category order inconsistency detected, automatically correcting...\nCorrected category order： ['Groove_billed_Ani', 'Red_winged_Blackbird', 'Rusty_Blackbird', 'Gray_Catbird', 'Brandt_Cormorant', 'Eastern_Towhee', 'Indigo_Bunting', 'Brewer_Blackbird', 'Painted_Bunting', 'Bobolink', 'Lazuli_Bunting', 'Yellow_headed_Blackbird', 'American_Crow', 'Fish_Crow', 'Brown_Creeper', 'Yellow_billed_Cuckoo', 'Yellow_breasted_Chat', 'Black_billed_Cuckoo', 'Gray_crowned_Rosy_Finch', 'Bronzed_Cowbird']\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"import os\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n\n# (Assuming train_dataset, val_dataset, model, optimizer, criterion, etc. are already defined)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = model.to(device)\n\nclass EarlyStopper:\n    def __init__(self, patience=5, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.min_validation_loss = float('inf')\n\n    def early_stop(self, validation_loss):\n        if validation_loss < self.min_validation_loss:\n            self.min_validation_loss = validation_loss\n            self.counter = 0\n        elif validation_loss > (self.min_validation_loss + self.min_delta):\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False\n\ndef train_epoch(model, loader, optimizer, scaler, scheduler):\n    model.train()\n    total_loss = 0\n    correct = 0\n    \n    for images, labels in loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        with torch.amp.autocast(device_type=\"cuda\"):\n            outputs = model(images)\n            logits = F.normalize(outputs.logits)  # Normalize logits\n            loss = criterion(logits, labels)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        total_loss += loss.item() * images.size(0)\n        preds = torch.argmax(logits, dim=1)  # Predictions after normalization\n        correct += (preds == labels).sum().item()\n    \n    avg_loss = total_loss / len(loader.dataset)\n    accuracy = correct / len(loader.dataset)\n    \n    scheduler.step()\n    \n    return avg_loss, accuracy\n\n\ndef validate(model, loader):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            with torch.amp.autocast(device_type=\"cuda\"):  # Mixed precision\n                outputs = model(images)\n                loss = criterion(outputs.logits, labels)\n            \n            total_loss += loss.item() * images.size(0)\n            preds = torch.argmax(outputs.logits, dim=1)\n            correct += (preds == labels).sum().item()\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    avg_loss = total_loss / len(loader.dataset)\n    accuracy = correct / len(loader.dataset)\n\n    # Compute Precision, Recall, and F1-score\n    precision = precision_score(all_labels, all_preds, average=\"macro\")\n    recall = recall_score(all_labels, all_preds, average=\"macro\")\n    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n\n    # Print detailed classification report\n    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))\n\n    return avg_loss, accuracy, precision, recall, f1\n\nscaler = torch.amp.GradScaler('cuda')\nearly_stopper = EarlyStopper(patience=10, min_delta=0.0003)\nhistory = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n\n# Variables to track the best validation accuracy\nbest_val_acc = 0.0\nbest_model_path = \"/kaggle/working/best_model.pth\"\n\nhistory = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [],\n           'val_precision': [], 'val_recall': [], 'val_f1': []}\n\nfor epoch in range(20):\n    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scaler, scheduler)\n    val_loss, val_acc, val_precision, val_recall, val_f1 = validate(model, val_loader)\n    \n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['train_acc'].append(train_acc)\n    history['val_acc'].append(val_acc)\n    history['val_precision'].append(val_precision)\n    history['val_recall'].append(val_recall)\n    history['val_f1'].append(val_f1)\n    \n    print(f\"Epoch {epoch+1:02d}:\")\n    print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\")\n    print(f\"Precision: {val_precision:.4f} | Recall: {val_recall:.4f} | F1-score: {val_f1:.4f}\\n\")\n    \n    # 保存最佳模型\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"New best model saved with validation accuracy: {val_acc:.4f}\")\n    \n    if early_stopper.early_stop(val_loss):\n        print(\"Early stopping triggered!\")\n        break\n\n# Plot training curves\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history['train_loss'], label='Train')\nplt.plot(history['val_loss'], label='Validation')\nplt.title('Loss Curve')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history['train_acc'], label='Train')\nplt.plot(history['val_acc'], label='Validation')\nplt.title('Accuracy Curve')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T13:45:19.531223Z","iopub.execute_input":"2025-02-06T13:45:19.531507Z"}},"outputs":[{"name":"stdout","text":"\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.86      1.00      0.92        12\n           1       0.92      1.00      0.96        12\n           2       0.60      0.75      0.67        12\n           3       0.92      1.00      0.96        12\n           4       1.00      0.27      0.43        11\n           5       0.86      1.00      0.92        12\n           6       1.00      1.00      1.00        12\n           7       0.56      0.42      0.48        12\n           8       1.00      0.75      0.86        12\n           9       1.00      1.00      1.00        12\n          10       0.80      1.00      0.89        12\n          11       1.00      0.82      0.90        11\n          12       0.53      0.67      0.59        12\n          13       0.22      0.17      0.19        12\n          14       1.00      1.00      1.00        12\n          15       1.00      0.17      0.29        12\n          16       0.85      1.00      0.92        11\n          17       0.57      1.00      0.73        12\n          18       1.00      0.92      0.96        12\n          19       0.73      1.00      0.85        11\n\n    accuracy                           0.80       236\n   macro avg       0.82      0.80      0.77       236\nweighted avg       0.82      0.80      0.77       236\n\nEpoch 01:\nTrain Loss: 2.7827 | Acc: 0.3223\nVal Loss: 0.6482 | Acc: 0.7966\nPrecision: 0.8211 | Recall: 0.7962 | F1-score: 0.7750\n\nNew best model saved with validation accuracy: 0.7966\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        12\n           1       1.00      1.00      1.00        12\n           2       0.67      0.83      0.74        12\n           3       1.00      1.00      1.00        12\n           4       1.00      0.91      0.95        11\n           5       1.00      1.00      1.00        12\n           6       1.00      1.00      1.00        12\n           7       0.75      0.50      0.60        12\n           8       1.00      1.00      1.00        12\n           9       1.00      1.00      1.00        12\n          10       1.00      1.00      1.00        12\n          11       1.00      1.00      1.00        11\n          12       0.52      0.92      0.67        12\n          13       0.50      0.08      0.14        12\n          14       1.00      1.00      1.00        12\n          15       1.00      0.67      0.80        12\n          16       0.92      1.00      0.96        11\n          17       0.75      1.00      0.86        12\n          18       1.00      1.00      1.00        12\n          19       0.85      1.00      0.92        11\n\n    accuracy                           0.89       236\n   macro avg       0.90      0.90      0.88       236\nweighted avg       0.90      0.89      0.88       236\n\nEpoch 02:\nTrain Loss: 2.4691 | Acc: 0.7407\nVal Loss: 0.3316 | Acc: 0.8941\nPrecision: 0.8977 | Recall: 0.8955 | F1-score: 0.8816\n\nNew best model saved with validation accuracy: 0.8941\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        12\n           1       1.00      1.00      1.00        12\n           2       0.69      0.92      0.79        12\n           3       1.00      1.00      1.00        12\n           4       1.00      1.00      1.00        11\n           5       1.00      1.00      1.00        12\n           6       1.00      1.00      1.00        12\n           7       0.89      0.67      0.76        12\n           8       1.00      1.00      1.00        12\n           9       1.00      1.00      1.00        12\n          10       1.00      1.00      1.00        12\n          11       1.00      1.00      1.00        11\n          12       0.55      0.92      0.69        12\n          13       0.67      0.17      0.27        12\n          14       1.00      1.00      1.00        12\n          15       0.90      0.75      0.82        12\n          16       1.00      1.00      1.00        11\n          17       0.79      0.92      0.85        12\n          18       1.00      1.00      1.00        12\n          19       1.00      1.00      1.00        11\n\n    accuracy                           0.92       236\n   macro avg       0.92      0.92      0.91       236\nweighted avg       0.92      0.92      0.91       236\n\nEpoch 03:\nTrain Loss: 2.3675 | Acc: 0.8111\nVal Loss: 0.2558 | Acc: 0.9153\nPrecision: 0.9239 | Recall: 0.9167 | F1-score: 0.9083\n\nNew best model saved with validation accuracy: 0.9153\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        12\n           1       1.00      1.00      1.00        12\n           2       0.67      0.83      0.74        12\n           3       1.00      1.00      1.00        12\n           4       1.00      1.00      1.00        11\n           5       1.00      1.00      1.00        12\n           6       1.00      1.00      1.00        12\n           7       0.80      0.67      0.73        12\n           8       1.00      1.00      1.00        12\n           9       1.00      1.00      1.00        12\n          10       1.00      1.00      1.00        12\n          11       1.00      1.00      1.00        11\n          12       0.55      0.92      0.69        12\n          13       0.67      0.17      0.27        12\n          14       1.00      1.00      1.00        12\n          15       0.91      0.83      0.87        12\n          16       1.00      1.00      1.00        11\n          17       0.85      0.92      0.88        12\n          18       1.00      1.00      1.00        12\n          19       1.00      1.00      1.00        11\n\n    accuracy                           0.92       236\n   macro avg       0.92      0.92      0.91       236\nweighted avg       0.92      0.92      0.91       236\n\nEpoch 04:\nTrain Loss: 2.3297 | Acc: 0.8335\nVal Loss: 0.2280 | Acc: 0.9153\nPrecision: 0.9219 | Recall: 0.9167 | F1-score: 0.9086\n\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\n\n# Define the directory where you'll save the best model in Hugging Face format.\nsave_dir = \"/kaggle/working/swin_model\"\nos.makedirs(save_dir, exist_ok=True)\n\n# Load the best model state dictionary (from the training loop).\nbest_state_dict = torch.load(best_model_path, map_location=torch.device(\"cpu\"))\nmodel.load_state_dict(best_state_dict)\n\n# Save the best model in Hugging Face format.\nmodel.save_pretrained(save_dir)\nprocessor.save_pretrained(save_dir)\n\n# Optionally, save additional information such as class mapping and training configuration.\nclass_info = {\n    \"class_order\": submission_class_order,\n    \"class_to_idx\": train_dataset.class_to_idx,\n    \"idx_to_class\": {v: k for k, v in train_dataset.class_to_idx.items()}\n}\ntorch.save(class_info, os.path.join(save_dir, \"class_info.pth\"))\n\ntrain_config = {\n    \"epochs_trained\": len(history['train_loss']),\n    \"best_val_acc\": max(history['val_acc']),\n    \"optimizer_state\": optimizer.state_dict()\n}\ntorch.save(train_config, os.path.join(save_dir, \"train_config.pth\"))\n\n# Also save the class order as a plain text file for reference.\nwith open(os.path.join(save_dir, \"class_info.txt\"), \"w\") as f:\n    f.write(\"\\n\".join(submission_class_order))\n\nprint(f\"Best model saved in Hugging Face format to: {save_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test and Generate Submission File","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom transformers import AutoImageProcessor, SwinForImageClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nsubmission_class_order = [\n    'Groove_billed_Ani',\n    'Red_winged_Blackbird',\n    'Rusty_Blackbird',\n    'Gray_Catbird',\n    'Brandt_Cormorant',\n    'Eastern_Towhee',\n    'Indigo_Bunting',\n    'Brewer_Blackbird',\n    'Painted_Bunting',\n    'Bobolink',\n    'Lazuli_Bunting',\n    'Yellow_headed_Blackbird',\n    'American_Crow',\n    'Fish_Crow',\n    'Brown_Creeper',\n    'Yellow_billed_Cuckoo',\n    'Yellow_breasted_Chat',\n    'Black_billed_Cuckoo',\n    'Gray_crowned_Rosy_Finch',\n    'Bronzed_Cowbird'\n]\n\nmodel_dir = \"/kaggle/working/swin_model\" \nprocessor = AutoImageProcessor.from_pretrained(model_dir)\n\nexpected_size = 384\nval_transform = transforms.Compose([\n    transforms.Resize((expected_size, expected_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n])\n\nclass CompetitionTestDataset(Dataset):\n    def __init__(self, test_dir, transform=None):\n        self.test_dir = test_dir\n        self.image_files = sorted(os.listdir(test_dir))  # 保持严格顺序\n        self.image_paths = [os.path.join(test_dir, f) for f in self.image_files]\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx]).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, os.path.basename(self.image_paths[idx])\n\ndef load_trained_model(model_dir, class_order):\n    model = SwinForImageClassification.from_pretrained(model_dir).to(device)\n    \n    config = model.config\n    if config.id2label != {i: cls for i, cls in enumerate(class_order)}:\n        print(\"Warning: The order of categories in the model configuration is inconsistent with the submission requirements. Overriding with submission order.\")\n        config.id2label = {i: cls for i, cls in enumerate(class_order)}\n        config.label2id = {cls: i for i, cls in enumerate(class_order)}\n    \n    model.eval()\n    return model\n\ndef generate_submission(test_dir, model_dir, output_csv=\"submission.csv\"):\n    test_dataset = CompetitionTestDataset(test_dir, transform=val_transform)\n    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n    \n    model = load_trained_model(model_dir, submission_class_order)\n    \n    filenames = []\n    predictions = []\n    \n    with torch.no_grad():\n        for images, paths in test_loader:\n            outputs = model(images.to(device))\n            batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n            filenames.extend(paths)\n            predictions.extend(batch_preds.tolist())\n    \n    submission_df = pd.DataFrame({\n        'path': filenames,\n        'class_idx': predictions\n    })\n    \n    print(\"\\nValidation Results:\")\n    print(f\"Total Samples: {len(submission_df)}\")\n    print(f\"Number of unique file names: {submission_df['path'].nunique()}\")\n    print(f\"Predicted category distribution:\\n{submission_df['class_idx'].value_counts().sort_index()}\")\n    \n    submission_df.to_csv(output_csv, index=False)\n    print(f\"\\nSubmission CSV saved to: {output_csv}\")\n\nif __name__ == \"__main__\":\n    test_dir = \"/kaggle/input/croped-processed-augmented-bird-dataset/archive/Augmented_CropImage_Processed_Dataset/test_images/mistery_cat\"\n    generate_submission(test_dir, model_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}